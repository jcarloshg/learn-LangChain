# ğŸ“ Best Practices & Tips for Managing Prompts in LangChain

> Templates, Few-shot Learning, and Selectors Mastery Guide

---

## ğŸ“‘ Table of Contents

### Part 1: Best Practices for Prompt Management

1. [Use Prompt Templates for Consistency and Reusability](#1-use-prompt-templates-for-consistency-and-reusability)
2. [Separate System Prompt from User Input](#2-separate-system-prompt-from-user-input)
3. [Implement Few-Shot Learning for Better Outputs](#3-implement-few-shot-learning-for-better-outputs)
4. [Use Prompt Selectors for Dynamic Few-Shot Examples](#4-use-prompt-selectors-for-dynamic-few-shot-examples)
5. [Version Control Your Prompts](#5-version-control-your-prompts)
6. [Use Prompt Composition for Complex Workflows](#6-use-prompt-composition-for-complex-workflows)
7. [Make Prompts Specific and Detailed](#7-make-prompts-specific-and-detailed)
8. [Include Output Format in Your Prompt](#8-include-output-format-in-your-prompt)
9. [Add Constraints and Guardrails to Prompts](#9-add-constraints-and-guardrails-to-prompts)
10. [Test and Iterate Your Prompts](#10-test-and-iterate-your-prompts)

### Part 2: Tips & Tricks for Prompts

1. [Use Role-Playing for Better Performance](#trick-1-use-role-playing-for-better-performance)
2. [Chain of Thought Prompting](#trick-2-chain-of-thought-prompting)
3. [Use Prompt Templates with Optional Sections](#trick-3-use-prompt-templates-with-optional-sections)
4. [Few-Shot with Negative Examples](#trick-4-few-shot-with-negative-examples)
5. [Dynamic Prompt Prefix/Suffix](#trick-5-dynamic-prompt-prefixsuffix)
6. [Prompt Decomposition](#trick-6-prompt-decomposition)
7. [Prompt Templating with Conditions](#trick-7-prompt-templating-with-conditions)
8. [Few-Shot with Example Weights](#trick-8-few-shot-with-example-weights)
9. [Prompt Combination/Merging](#trick-9-prompt-combinationmerging)
10. [Prompt Personas for Specific Outputs](#trick-10-prompt-personas-for-specific-outputs)
11. [Iterative Prompt Refinement](#trick-11-iterative-prompt-refinement)
12. [Prompt Caching to Reduce Costs](#trick-12-prompt-caching-to-reduce-costs)

### Reference & Checklists

- [Quick Reference: Prompt Types by Use Case](#-quick-reference-prompt-types-by-use-case)
- [Prompt Engineering Checklist](#-prompt-engineering-checklist)
- [Common Prompt Mistakes to Avoid](#-common-prompt-mistakes-to-avoid)
- [Production Prompt Checklist](#-production-prompt-checklist)

---

## ğŸ“‹ Part 1: Best Practices for Prompt Management

### 1. **Use Prompt Templates for Consistency and Reusability**

**Why:** Hardcoded prompts are a nightmare. Templates make prompts reusable, maintainable, and version-controllable.

```python
from langchain_core.prompts import ChatPromptTemplate

# âŒ Bad: Hardcoded strings scattered everywhere
response = llm.invoke(f"Summarize this: {document}")

# âœ… Good: Reusable, testable template
summarize_template = ChatPromptTemplate.from_template(
    "Summarize the following document in 3 sentences:\n\n{document}"
)

# Can be used everywhere
result1 = (summarize_template | llm).invoke({"document": doc1})
result2 = (summarize_template | llm).invoke({"document": doc2})
```

**Template Benefits:**

- Consistency across your application
- Easy to modify (change once, updates everywhere)
- Testable and version-controllable
- Clear input/output contracts

---

### 2. **Separate System Prompt from User Input**

**Why:** Different roles have different purposes. System sets behavior, user asks questions.

```python
from langchain_core.prompts import ChatPromptTemplate

# âœ… Best practice: Separate system and user prompts
template = ChatPromptTemplate.from_messages([
    ("system", """You are an expert Python developer.
Your role is to:
- Provide clear, efficient code
- Explain concepts thoroughly
- Follow Python best practices
- Use type hints
"""),
    ("user", "{user_question}")
])

chain = template | llm
result = chain.invoke({"user_question": "How do I use decorators?"})

# Benefits:
# 1. System prompt stays consistent
# 2. Easy to swap different system personas
# 3. Clear separation of concerns
# 4. Reusable across many user inputs
```

**System Prompt Best Practices:**

```python
# âŒ Weak system prompt
system = "You are helpful."

# âœ… Strong system prompt
system = """You are an expert Python developer with 10+ years of experience.
Your role is to provide high-quality code solutions.

Guidelines:
- Always include type hints
- Write clean, readable code
- Explain your reasoning
- Suggest improvements and alternatives
- Consider edge cases and error handling

Output format:
- Code block with syntax highlighting
- Brief explanation of the approach
- Potential improvements or pitfalls"""

template = ChatPromptTemplate.from_messages([
    ("system", system),
    ("user", "{question}")
])
```

---

### 3. **Implement Few-Shot Learning for Better Outputs**

**Why:** Examples teach the model exactly what you want. Reduces mistakes and hallucinations.

```python
from langchain_core.prompts import FewShotChatMessagePromptTemplate, ChatPromptTemplate

# Define clear, representative examples
examples = [
    {
        "input": "Apple Inc. employs 161,000 people",
        "output": '{"company": "Apple Inc.", "employees": 161000}'
    },
    {
        "input": "Microsoft has 221,000 employees",
        "output": '{"company": "Microsoft", "employees": 221000}'
    },
    {
        "input": "Google employs 190,234 people worldwide",
        "output": '{"company": "Google", "employees": 190234}'
    }
]

# Create few-shot template
few_shot_prompt = FewShotChatMessagePromptTemplate(
    examples=examples,
    example_prompt=ChatPromptTemplate.from_messages([
        ("user", "{input}"),
        ("assistant", "{output}")
    ]),
    suffix="Extract company info from: {user_input}",
    input_variables=["user_input"]
)

chain = few_shot_prompt | llm
result = chain.invoke({"user_input": "Tesla has 127,855 employees"})
```

**Few-Shot Best Practices:**

```python
# 1. Use representative examples (cover edge cases)
examples = [
    {"text": "Great!", "sentiment": "positive"},      # Normal positive
    {"text": "Best ever!", "sentiment": "positive"},  # Enthusiastic
    {"text": "Not bad", "sentiment": "neutral"},      # Ambiguous
    {"text": "Terrible", "sentiment": "negative"},    # Strong negative
    {"text": "Meh", "sentiment": "negative"},         # Weak negative
]

# 2. Use diverse examples (different formats, lengths)
examples = [
    {"sentence": "The dog ran quickly.", "label": "simple"},
    {"sentence": "Because of the rain, they decided to cancel.", "label": "complex"},
    {"sentence": "It was great!", "label": "simple"},
]

# 3. Keep examples consistent in quality and format
examples = [
    {
        "input": "Summarize: AI is transforming healthcare",
        "output": "AI transforms healthcare"  # Consistent, concise
    },
    {
        "input": "Summarize: Climate change affects weather patterns",
        "output": "Climate change affects weather"  # Same style
    }
]
```

---

### 4. **Use Prompt Selectors for Dynamic Few-Shot Examples**

**Why:** Different queries need different examples. Selectors pick the most relevant ones.

```python
from langchain_core.prompts import FewShotChatMessagePromptTemplate, ChatPromptTemplate
from langchain_core.example_selectors import SemanticSimilarityExampleSelector
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS

# Prepare examples
examples = [
    {"input": "Happy people enjoy life", "label": "positive"},
    {"input": "Sad movies make people cry", "label": "negative"},
    {"input": "Neutral statements are factual", "label": "neutral"},
    {"input": "Angry customers complain", "label": "negative"},
    {"input": "Excited fans cheer loudly", "label": "positive"},
]

# Create example selector based on semantic similarity
example_selector = SemanticSimilarityExampleSelector.from_examples(
    examples,
    OpenAIEmbeddings(),
    FAISS,
    k=2  # Select 2 most similar examples
)

# Use in prompt template
few_shot_prompt = FewShotChatMessagePromptTemplate(
    example_selector=example_selector,
    example_prompt=ChatPromptTemplate.from_messages([
        ("user", "{input}"),
        ("assistant", "{label}")
    ]),
    suffix="Classify sentiment: {user_input}",
    input_variables=["user_input"]
)

chain = few_shot_prompt | llm

# Different inputs get different examples automatically
result1 = chain.invoke({"user_input": "I love this!"})  # Gets positive examples
result2 = chain.invoke({"user_input": "This is terrible"})  # Gets negative examples
```

---

### 5. **Version Control Your Prompts**

**Why:** Prompts change over time. You need to track which prompt version produces best results.

```python
import json
from datetime import datetime
from typing import Dict

class PromptVersionManager:
    def __init__(self, filepath: str = "prompts.json"):
        self.filepath = filepath
        self.versions = {}
        self.load()

    def save_prompt(self, name: str, content: str, metadata: Dict = None):
        """Save a prompt version"""
        version = {
            "content": content,
            "created_at": datetime.now().isoformat(),
            "metadata": metadata or {}
        }

        if name not in self.versions:
            self.versions[name] = []

        self.versions[name].append(version)
        self.save()

    def get_latest(self, name: str) -> str:
        """Get the latest version of a prompt"""
        return self.versions[name][-1]["content"]

    def get_version(self, name: str, version_num: int) -> str:
        """Get a specific version"""
        return self.versions[name][version_num]["content"]

    def compare_versions(self, name: str, v1: int, v2: int):
        """Compare two versions"""
        print(f"Version {v1}: {self.versions[name][v1]['content']}")
        print(f"\nVersion {v2}: {self.versions[name][v2]['content']}")

    def save(self):
        with open(self.filepath, 'w') as f:
            json.dump(self.versions, f, indent=2)

    def load(self):
        try:
            with open(self.filepath, 'r') as f:
                self.versions = json.load(f)
        except FileNotFoundError:
            self.versions = {}

# Usage
manager = PromptVersionManager()

# V1: Initial prompt
manager.save_prompt(
    "summarizer",
    "Summarize in 3 sentences: {text}",
    {"author": "alice", "performance": "70%"}
)

# V2: Improved prompt
manager.save_prompt(
    "summarizer",
    "Summarize in 2-3 sentences, keeping key facts: {text}",
    {"author": "bob", "performance": "85%"}
)

# V3: Further improvement
manager.save_prompt(
    "summarizer",
    """Summarize the following text in 2-3 sentences.
Focus on main ideas and key facts.
Omit minor details.
Text: {text}""",
    {"author": "alice", "performance": "92%"}
)

# Use the best version
latest = manager.get_latest("summarizer")
v1 = manager.get_version("summarizer", 0)
```

---

### 6. **Use Prompt Composition for Complex Workflows**

**Why:** Complex tasks need multiple prompts. Composition keeps them organized.

```python
from langchain_core.prompts import ChatPromptTemplate

# Step 1: Analysis prompt
analysis_prompt = ChatPromptTemplate.from_template(
    "Analyze the main points in: {document}"
)

# Step 2: Summarization prompt
summary_prompt = ChatPromptTemplate.from_template(
    "Summarize these points in 2 sentences: {points}"
)

# Step 3: Translation prompt
translate_prompt = ChatPromptTemplate.from_template(
    "Translate to Spanish: {summary}"
)

# Compose into chain
chain = (
    analysis_prompt
    | llm
    | summary_prompt
    | llm
    | translate_prompt
    | llm
)

result = chain.invoke({
    "document": "Your long document here...",
    "points": "Extracted from previous step",
    "summary": "Generated from previous step"
})
```

---

### 7. **Make Prompts Specific and Detailed**

**Why:** Vague prompts = vague outputs. Specific prompts = predictable, quality outputs.

```python
# âŒ Too vague
vague = "Explain machine learning"

# âœ… Specific with context
specific = """Explain machine learning to a beginner with no technical background.
Include:
1. Simple definition (1 sentence)
2. Real-world example (1 paragraph)
3. How it differs from traditional programming (1 paragraph)
Use simple language, avoid jargon."""

# âœ… Even better: Specify output format
detailed = """Explain machine learning to a beginner with no technical background.

Output format:
**Definition**: [One sentence explaining ML]
**Real-world example**: [1 paragraph with concrete example]
**vs Traditional**: [1 paragraph comparing to regular programming]
**Key takeaway**: [One sentence summary]

Requirements:
- Use simple language
- Avoid technical jargon
- Include relatable examples"""
```

---

### 8. **Include Output Format in Your Prompt**

**Why:** Explicit formatting prevents parsing errors and ensures consistency.

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field

# Define expected output structure
class MovieReview(BaseModel):
    title: str = Field(description="Movie title")
    rating: int = Field(description="Rating 1-10")
    pros: list = Field(description="What was good")
    cons: list = Field(description="What was bad")
    recommendation: str = Field(description="Recommend? yes/no")

# Create prompt with explicit format
template = ChatPromptTemplate.from_template(
    """Review this movie: {movie_description}

Output as JSON with this structure:
{{
    "title": "movie title",
    "rating": 8,
    "pros": ["good acting", "great plot"],
    "cons": ["slow pacing"],
    "recommendation": "yes"
}}"""
)

# Combine with parser
parser = JsonOutputParser(pydantic_object=MovieReview)
chain = template | llm | parser

result = chain.invoke({"movie_description": "Inception is a mind-bending sci-fi..."})
# result is validated MovieReview object
```

---

### 9. **Add Constraints and Guardrails to Prompts**

**Why:** Prevents unwanted behavior, ensures safety, controls output size.

```python
from langchain_core.prompts import ChatPromptTemplate

# Prompt with constraints
constrained_prompt = ChatPromptTemplate.from_template(
    """You are a helpful assistant.

CONSTRAINTS:
- Response must be under 150 words
- Do NOT provide medical advice
- Do NOT discuss illegal activities
- Be respectful and professional
- If unsure, say "I don't know"

User question: {question}

Please answer following the constraints above."""
)

# Prompt with safety guardrails
safe_prompt = ChatPromptTemplate.from_template(
    """You are an educational AI assistant.

GUARDRAILS:
1. Only discuss factual, educational topics
2. Decline requests for:
   - Hacking or cybercrime
   - Hate speech or discrimination
   - Personal information of others
   - Copyright violation assistance
3. If a request violates these, politely decline

User request: {request}

Respond while following all guardrails."""
)
```

---

### 10. **Test and Iterate Your Prompts**

**Why:** Small prompt changes = big output differences. Testing finds optimal prompts.

```python
from typing import List
import json

class PromptTester:
    def __init__(self, llm, test_cases: List[dict]):
        self.llm = llm
        self.test_cases = test_cases
        self.results = []

    def test_prompt(self, prompt_template: str, name: str):
        """Test prompt against all test cases"""
        scores = []

        for test in self.test_cases:
            # Format prompt
            formatted = prompt_template.format(**test["input"])

            # Get response
            response = self.llm.invoke(formatted).content

            # Score (you implement your scoring logic)
            score = self.score_response(response, test["expected"])
            scores.append(score)

        average = sum(scores) / len(scores)
        self.results.append({
            "name": name,
            "scores": scores,
            "average": average
        })

        return average

    def score_response(self, response: str, expected: str) -> float:
        """Simple scoring: 1 if match, 0 if not"""
        return 1.0 if expected.lower() in response.lower() else 0.0

    def get_best(self) -> dict:
        """Get best performing prompt"""
        return max(self.results, key=lambda x: x["average"])

    def print_report(self):
        """Print test results"""
        for result in sorted(self.results, key=lambda x: x["average"], reverse=True):
            print(f"{result['name']}: {result['average']:.2%}")

# Usage
test_cases = [
    {"input": {"text": "The cat sat"}, "expected": "cat"},
    {"input": {"text": "Dogs run fast"}, "expected": "dog"},
    {"input": {"text": "Birds fly high"}, "expected": "bird"},
]

tester = PromptTester(llm, test_cases)

# Test different prompts
tester.test_prompt("Extract animal: {text}", "v1_simple")
tester.test_prompt("Extract the animal mentioned: {text}. Just say the animal.", "v2_explicit")
tester.test_prompt("Extract the main animal from: {text}. Format: ANIMAL: [name]", "v3_formatted")

# See which is best
tester.print_report()
best = tester.get_best()
print(f"\nBest prompt: {best['name']}")
```

---

## ğŸ¯ Part 2: Tips & Tricks for Prompts

### Trick 1: **Use Role-Playing for Better Performance**

Give the LLM a role to adopt. It significantly improves output quality.

```python
from langchain_core.prompts import ChatPromptTemplate

# Without role
basic = "Explain quantum computing"

# With role - much better results
expert_role = ChatPromptTemplate.from_messages([
    ("system", """You are a Nobel Prize-winning physicist specializing in quantum mechanics.
    You explain complex concepts with clarity and precision.
    You use analogies and real-world examples."""),
    ("user", "Explain quantum computing to an undergraduate student")
])

# Different roles for different purposes
teacher_role = ChatPromptTemplate.from_messages([
    ("system", "You are a patient, encouraging high school teacher."),
    ("user", "{question}")
])

coder_role = ChatPromptTemplate.from_messages([
    ("system", "You are a senior software engineer with 15 years of experience."),
    ("user", "{question}")
])

# Use appropriate role for task
physics_answer = (expert_role | llm).invoke({})
teaching_answer = (teacher_role | llm).invoke({"question": "What is photosynthesis?"})
code_answer = (coder_role | llm).invoke({"question": "How do I optimize this code?"})
```

---

### Trick 2: **Chain of Thought Prompting**

Force the model to think step-by-step. Dramatically improves reasoning.

```python
from langchain_core.prompts import ChatPromptTemplate

# Without CoT - model may guess
simple = "Is 17 * 23 + 45 greater than 500?"

# With CoT - model explains reasoning
chain_of_thought = ChatPromptTemplate.from_template("""
Solve this step by step:

Problem: {problem}

Solution steps:
1. [First step]
2. [Second step]
3. [Final answer]

Work through it carefully.""")

result = (chain_of_thought | llm).invoke({
    "problem": "Is 17 * 23 + 45 greater than 500?"
})

# Even better: explicitly request reasoning
explicit_reasoning = ChatPromptTemplate.from_template("""
{problem}

Before answering:
1. Break down the problem
2. Solve each part
3. Combine the results
4. Verify your answer

Answer: [your answer]
Reasoning: [explain your thinking]""")
```

---

### Trick 3: **Use Prompt Templates with Optional Sections**

Dynamically include/exclude content based on context.

```python
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate

class DynamicPromptTemplate:
    def __init__(self):
        self.base = """Analyze the following: {text}"""
        self.with_examples = """{base}

Examples of good analysis:
{examples}"""
        self.with_constraints = """{base}

Constraints:
{constraints}"""

    def build(self, include_examples=False, include_constraints=False):
        template = self.base

        if include_examples:
            template = self.with_examples

        if include_constraints:
            if include_examples:
                # Combine both
                template = """{base}

Examples:
{examples}

Constraints:
{constraints}"""
            else:
                template = self.with_constraints

        return template

# Usage
builder = DynamicPromptTemplate()

# Simple prompt
simple_template = builder.build()

# With examples
with_examples_template = builder.build(include_examples=True)

# With everything
full_template = builder.build(include_examples=True, include_constraints=True)
```

---

### Trick 4: **Few-Shot with Negative Examples**

Include examples of what NOT to do.

```python
from langchain_core.prompts import FewShotChatMessagePromptTemplate, ChatPromptTemplate

examples = [
    # Positive examples
    {
        "input": "I love this product!",
        "type": "good_sentiment_example",
        "output": "positive"
    },
    {
        "input": "This is terrible",
        "type": "good_sentiment_example",
        "output": "negative"
    },
    # Negative examples (what NOT to do)
    {
        "input": "The product exists",
        "type": "bad_sentiment_example",
        "output": "NOT neutral - this is factual, not emotional"
    },
    {
        "input": "I think therefore I am",
        "type": "bad_sentiment_example",
        "output": "NOT a sentiment - this is philosophical"
    }
]

few_shot = FewShotChatMessagePromptTemplate(
    examples=examples,
    example_prompt=ChatPromptTemplate.from_messages([
        ("user", "{input}"),
        ("assistant", "[{type}] {output}")
    ]),
    suffix="Classify sentiment: {user_input}"
)

# Model learns both what to do AND what NOT to do
```

---

### Trick 5: **Dynamic Prompt Prefix/Suffix**

Add context dynamically based on user input.

```python
from langchain_core.prompts import ChatPromptTemplate

class ContextualPrompt:
    def __init__(self):
        self.base_template = """You are a helpful assistant.

Current context:
{context}

User question: {question}

Answer:"""

    def generate_context(self, user_input: str) -> str:
        """Generate context based on user input"""
        context_map = {
            "python": "You specialize in Python programming",
            "javascript": "You specialize in JavaScript development",
            "data": "You specialize in data science and analysis",
        }

        # Detect topic
        for topic, context in context_map.items():
            if topic in user_input.lower():
                return context

        return "You are a general knowledge assistant"

    def create_prompt(self, question: str):
        context = self.generate_context(question)
        return self.base_template.format(context=context, question=question)

# Usage
prompt_builder = ContextualPrompt()

# Question about Python â†’ gets Python specialist context
py_prompt = prompt_builder.create_prompt("How do decorators work in Python?")

# Question about data â†’ gets data science context
data_prompt = prompt_builder.create_prompt("How do I analyze data with pandas?")
```

---

### Trick 6: **Prompt Decomposition**

Break complex tasks into multiple simpler prompts.

```python
from langchain_core.prompts import ChatPromptTemplate

class ComplexTaskPrompts:
    @staticmethod
    def analyze_document(doc):
        """Decompose document analysis into steps"""
        # Step 1: Extract key information
        extract_prompt = ChatPromptTemplate.from_template(
            "Extract main topics from: {document}"
        )
        topics = (extract_prompt | llm).invoke({"document": doc})

        # Step 2: Summarize each topic
        summarize_prompt = ChatPromptTemplate.from_template(
            "Summarize this topic: {topic}"
        )
        summaries = {}
        for topic in topics.content.split('\n'):
            summary = (summarize_prompt | llm).invoke({"topic": topic})
            summaries[topic] = summary.content

        # Step 3: Create final analysis
        analyze_prompt = ChatPromptTemplate.from_template(
            "Create a final analysis from these summaries: {summaries}"
        )
        final = (analyze_prompt | llm).invoke({"summaries": str(summaries)})

        return final.content

# One complex task â†’ multiple simple prompts
result = ComplexTaskPrompts.analyze_document("Your complex document...")
```

---

### Trick 7: **Prompt Templating with Conditions**

Use conditionals to change prompt behavior.

```python
from langchain_core.prompts import ChatPromptTemplate

def create_conditional_prompt(user_level: str, topic: str):
    """Create prompt based on user expertise level"""

    if user_level == "beginner":
        template = """Explain {topic} in simple terms.
Use everyday examples.
Avoid technical jargon.
Make it fun and engaging."""

    elif user_level == "intermediate":
        template = """Explain {topic} with moderate detail.
Include technical concepts but explain them.
Provide practical examples.
Suggest next steps for learning."""

    else:  # advanced
        template = """Provide an in-depth explanation of {topic}.
Include theoretical foundations.
Discuss edge cases and advanced techniques.
Reference research or best practices."""

    return ChatPromptTemplate.from_template(template)

# Adjust explanation depth automatically
beginner_prompt = create_conditional_prompt("beginner", "Machine Learning")
pro_prompt = create_conditional_prompt("advanced", "Machine Learning")

beginner_answer = (beginner_prompt | llm).invoke({"topic": "Machine Learning"})
pro_answer = (pro_prompt | llm).invoke({"topic": "Machine Learning"})
```

---

### Trick 8: **Few-Shot with Example Weights**

Give more importance to certain examples.

```python
from langchain_core.prompts import FewShotChatMessagePromptTemplate, ChatPromptTemplate

class WeightedExampleSelector:
    def __init__(self, examples: list, weights: list = None):
        self.examples = examples
        if weights is None:
            weights = [1.0] * len(examples)
        self.weights = weights

    def select_examples(self, input_text: str, k: int = 3):
        """Select examples weighted by importance"""
        # Higher weight = more likely to be selected
        import random
        selected = random.choices(
            self.examples,
            weights=self.weights,
            k=k
        )
        return selected

# Examples with weights (high quality examples get higher weight)
examples = [
    {"input": "cat", "output": "animal", "weight": 2.0},  # High quality
    {"input": "dog", "output": "animal", "weight": 2.0},  # High quality
    {"input": "thing", "output": "object", "weight": 1.0},  # Lower quality
    {"input": "stuff", "output": "object", "weight": 0.5},  # Poor quality
]

selector = WeightedExampleSelector(
    examples,
    weights=[e["weight"] for e in examples]
)

# Better examples get selected more often
selected = selector.select_examples("bird", k=2)
```

---

### Trick 9: **Prompt Combination/Merging**

Combine multiple prompts for complex workflows.

```python
from langchain_core.prompts import ChatPromptTemplate

# Individual prompts
research_prompt = ChatPromptTemplate.from_template(
    "Research and list facts about: {topic}"
)

organize_prompt = ChatPromptTemplate.from_template(
    "Organize these facts into categories: {facts}"
)

creative_prompt = ChatPromptTemplate.from_template(
    "Create an interesting summary from: {organized}"
)

# Merge into workflow
def research_and_summarize(topic):
    # Step 1: Research
    facts = (research_prompt | llm).invoke({"topic": topic})

    # Step 2: Organize
    organized = (organize_prompt | llm).invoke({"facts": facts.content})

    # Step 3: Summarize creatively
    summary = (creative_prompt | llm).invoke({"organized": organized.content})

    return summary.content

# Use merged workflow
result = research_and_summarize("Artificial Intelligence")
```

---

### Trick 10: **Prompt Personas for Specific Outputs**

Create different "personas" for different output styles.

```python
from langchain_core.prompts import ChatPromptTemplate

# Different personas
personas = {
    "technical": """You are a technical expert. Provide:
- Precise terminology
- Technical accuracy
- Code examples when relevant
- Edge cases and considerations""",

    "sales": """You are a sales professional. Provide:
- Benefits and value propositions
- Clear advantages
- Compelling reasons to use
- Call to action""",

    "creative": """You are a creative writer. Provide:
- Engaging narrative
- Interesting metaphors
- Compelling examples
- Evocative language""",

    "educator": """You are a teacher. Provide:
- Clear explanations
- Relatable examples
- Step-by-step guidance
- Encouraging tone"""
}

def explain_with_persona(topic: str, persona: str):
    system_msg = personas.get(persona, personas["technical"])

    template = ChatPromptTemplate.from_messages([
        ("system", system_msg),
        ("user", "Explain {topic}")
    ])

    return (template | llm).invoke({"topic": topic})

# Same topic, different perspectives
technical = explain_with_persona("Machine Learning", "technical")
sales = explain_with_persona("Machine Learning", "sales")
creative = explain_with_persona("Machine Learning", "creative")
educational = explain_with_persona("Machine Learning", "educator")
```

---

### Trick 11: **Iterative Prompt Refinement**

Start simple, gradually add complexity based on results.

```python
class PromptIterator:
    def __init__(self, llm, base_prompt: str):
        self.llm = llm
        self.prompts = [base_prompt]

    def refine(self, addition: str):
        """Add more detail to prompt"""
        latest = self.prompts[-1]
        refined = f"{latest}\n\n{addition}"
        self.prompts.append(refined)
        return refined

    def test_current(self, test_input: str):
        """Test current prompt version"""
        latest = self.prompts[-1]
        result = self.llm.invoke(latest + f"\n\n{test_input}")
        return result.content

    def rollback(self):
        """Go back to previous version"""
        if len(self.prompts) > 1:
            self.prompts.pop()
            return self.prompts[-1]

    def get_history(self):
        """See all versions"""
        return self.prompts

# Usage: Iteratively improve prompt
iterator = PromptIterator(llm, "Summarize: {text}")

# Version 1
iterator.test_current("The quick brown fox...")

# Refine
iterator.refine("Keep it under 20 words")
iterator.test_current("The quick brown fox...")

# Refine more
iterator.refine("Use bullet points")
iterator.test_current("The quick brown fox...")

# See history
print(iterator.get_history())
```

---

### Trick 12: **Prompt Caching to Reduce Costs**

Cache prompts that don't change often.

```python
import hashlib
import json

class PromptCache:
    def __init__(self):
        self.cache = {}

    def get_cache_key(self, prompt_template: str, variables: dict) -> str:
        """Create cache key"""
        content = f"{prompt_template}{json.dumps(variables)}"
        return hashlib.md5(content.encode()).hexdigest()

    def get_or_execute(self, prompt_template: str, variables: dict, llm):
        """Execute prompt if not cached"""
        cache_key = self.get_cache_key(prompt_template, variables)

        if cache_key in self.cache:
            print(f"Cache hit! Returning cached result")
            return self.cache[cache_key]

        # Execute
        formatted = prompt_template.format(**variables)
        result = llm.invoke(formatted).content

        # Cache
        self.cache[cache_key] = result
        print(f"Cached new result")
        return result

    def clear(self):
        """Clear cache"""
        self.cache.clear()

# Usage
cache = PromptCache()

template = "Explain {concept}"
variables = {"concept": "quantum computing"}

# First call: executes
result1 = cache.get_or_execute(template, variables, llm)

# Second call: uses cache
result2 = cache.get_or_execute(template, variables, llm)

# Same result, no API call
```

---

## ğŸ“Š Quick Reference: Prompt Types by Use Case

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Use Case         â”‚ Best Prompt Type       â”‚ Key Technique               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Data Extraction  â”‚ Few-shot + structured  â”‚ Clear examples + JSON       â”‚
â”‚ Classification   â”‚ Few-shot + selectors   â”‚ Diverse examples            â”‚
â”‚ Summarization    â”‚ Template + constraints â”‚ Word limits, bullet points  â”‚
â”‚ Creative Work    â”‚ Persona-based          â”‚ Role-playing instructions   â”‚
â”‚ Problem Solving  â”‚ Chain of Thought       â”‚ Step-by-step reasoning      â”‚
â”‚ Code Generation  â”‚ Few-shot + detailed    â”‚ Implementation examples     â”‚
â”‚ Q&A              â”‚ Template + context     â”‚ Background info provided    â”‚
â”‚ Translation      â”‚ Few-shot               â”‚ Language pair examples      â”‚
â”‚ Analysis         â”‚ Decomposed prompts     â”‚ Multiple reasoning steps    â”‚
â”‚ Brainstorming    â”‚ Open-ended + persona   â”‚ Creative role, no limits    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Prompt Engineering Checklist

### Before Finalizing Your Prompt

- [ ] Is the goal clear and specific?
- [ ] Did you include system + user roles?
- [ ] Are examples representative of real cases?
- [ ] Is output format explicitly specified?
- [ ] Did you add relevant constraints?
- [ ] Did you test multiple versions?
- [ ] Does it handle edge cases?
- [ ] Is it concise but detailed?
- [ ] Can it be reused in other contexts?
- [ ] Is it version controlled?

### Few-Shot Checklist

- [ ] Examples are diverse and representative
- [ ] Examples use consistent formatting
- [ ] 3-5 examples (more isn't always better)
- [ ] Examples match expected difficulty level
- [ ] Include edge cases or ambiguous inputs
- [ ] Balance positive/negative examples
- [ ] Examples are high quality

### Template Checklist

- [ ] All variables are clearly marked: {variable}
- [ ] Variables have meaningful names
- [ ] Template is tested with real data
- [ ] Special characters are escaped
- [ ] Prompt length is reasonable
- [ ] Tone and style are consistent

---

## ğŸ¯ Common Prompt Mistakes to Avoid

| Mistake                       | Why It's Bad                        | Fix                                       |
| ----------------------------- | ----------------------------------- | ----------------------------------------- |
| Vague instructions            | Model guesses what you want         | Be specific and detailed                  |
| No output format              | Inconsistent, unpredictable results | Specify exact format (JSON, bullets, etc) |
| Too many examples             | Model gets confused, slower         | Use 3-5 good examples                     |
| Poor examples                 | Model learns wrong behavior         | Use high-quality, representative examples |
| Hardcoded prompts             | Can't reuse, hard to maintain       | Use prompt templates                      |
| No constraints                | Model may ramble or go off-topic    | Add length limits and guardrails          |
| Same system prompt everywhere | Inconsistent behavior               | Vary persona based on task                |
| Not testing                   | Ship broken prompts                 | Test multiple versions                    |

---

## ğŸš€ Production Prompt Checklist

- [ ] Prompts stored in templates (not hardcoded)
- [ ] Version control for all prompts
- [ ] Few-shot examples for critical tasks
- [ ] Output format explicitly specified
- [ ] Constraints and guardrails in place
- [ ] Tested against real test cases
- [ ] Monitoring of prompt performance
- [ ] System prompt clearly defined
- [ ] Fallback behavior for failures
- [ ] Documentation of why each prompt is used
- [ ] A/B testing different prompt versions
- [ ] Periodic prompt review and optimization

---

**Keep engineering better prompts. Keep iterating. Build smarter LLM applications.** ğŸš€

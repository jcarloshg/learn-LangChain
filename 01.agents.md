# ğŸ§  Best Practices & Tips for Managing Models in LangChain

> LLMs, Chat Models, and Embeddings Mastery Guide

---

## ğŸ“‘ Quick Index

### Part 1: Best Practices for Model Management

1. [Use Environment Variables for API Keys](#1-use-environment-variables-for-api-keys)
2. [Select the Right Model for Your Task](#2-select-the-right-model-for-your-task)
3. [Implement Proper Error Handling & Retries](#3-implement-proper-error-handling--retries)
4. [Monitor Token Usage and Costs](#4-monitor-token-usage-and-costs)
5. [Use Temperature & Top-P Strategically](#5-use-temperature--top-p-strategically)
6. [Implement Context Window Management](#6-implement-context-window-management)
7. [Cache Embeddings & Responses](#7-cache-embeddings--responses)
8. [Use Structured Outputs (Modern Models)](#8-use-structured-outputs-modern-models)
9. [Choose Right LLM Provider for Your Needs](#9-choose-right-llm-provider-for-your-needs)
10. [Implement Monitoring & Logging](#10-implement-monitoring--logging)

### Part 2: Tips & Tricks for Models

1. [Parallel Model Calls for Comparison](#trick-1-parallel-model-calls-for-comparison)
2. [Model Fallback Chain](#trick-2-model-fallback-chain)
3. [Dynamic Temperature Based on Task Complexity](#trick-3-dynamic-temperature-based-on-task-complexity)
4. [Batch Embed Multiple Texts Efficiently](#trick-4-batch-embed-multiple-texts-efficiently)
5. [Rate Limiting to Avoid API Throttling](#trick-5-rate-limiting-to-avoid-api-throttling)
6. [Use Model Aliases for Easy Switching](#trick-6-use-model-aliases-for-easy-switching)
7. [Stream Responses for Real-Time UX](#trick-7-stream-responses-for-real-time-ux)
8. [Create Model Ensembles](#trick-8-create-model-ensembles)
9. [Adaptive Model Selection Based on Input Length](#trick-9-adaptive-model-selection-based-on-input-length)
10. [Create a Model Performance Benchmark](#trick-10-create-a-model-performance-benchmark)
11. [Use System Messages Effectively](#trick-11-use-system-messages-effectively)
12. [Implement Smart Caching with TTL](#trick-12-implement-smart-caching-with-ttl)

### Quick Reference

- [Model Selection Matrix](#-quick-reference-model-selection-matrix)
- [Production Checklist](#-production-checklist)

---

## ğŸ“‹ Part 1: Best Practices for Model Management

### 1. **Use Environment Variables for API Keys**

**Why:** Security first. Never hardcode credentials.

```python
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

load_dotenv()  # Load from .env file

# Safe approach
api_key = os.getenv("OPENAI_API_KEY")
llm = ChatOpenAI(api_key=api_key, model="gpt-4")

# Even better: LangChain auto-loads from environment
llm = ChatOpenAI(model="gpt-4")  # Automatically reads OPENAI_API_KEY
```

**Best Practice Checklist:**

- [ ] Store API keys in `.env` file (add to `.gitignore`)
- [ ] Use `python-dotenv` to load environment variables
- [ ] Never commit `.env` to version control
- [ ] Use different keys for dev/staging/production
- [ ] Rotate keys periodically

---

### 2. **Select the Right Model for Your Task**

**Why:** Wrong model choice = wasted money, poor performance, or unnecessary latency.

| Use Case                 | Recommended Model                            | Why                        |
| ------------------------ | -------------------------------------------- | -------------------------- |
| **Real-time chat apps**  | GPT-4o mini, Claude 3.5 Haiku                | Fast, cheap, low latency   |
| **Complex reasoning**    | GPT-4, Claude 3.5 Opus                       | Better reasoning, accurate |
| **Long context**         | Claude 3.5 Sonnet (200k), GPT-4 Turbo (128k) | Handles large documents    |
| **Cost-sensitive tasks** | GPT-4o mini, Llama 3 (local)                 | Cheap per token            |
| **Local/private**        | Ollama (Llama 2, Mistral)                    | No API calls, full control |
| **Embeddings**           | text-embedding-3-small                       | Cheap, fast, 1536 dims     |
| **Semantic search**      | text-embedding-3-large                       | Better quality, 3072 dims  |

```python
from langchain_openai import ChatOpenAI
from langchain_community.llms import Ollama

# For fast, cheap responses
fast_model = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)

# For complex reasoning
smart_model = ChatOpenAI(model="gpt-4", temperature=0.5)

# For local privacy
local_model = Ollama(model="mistral")

# Switch based on task difficulty
def get_model(complexity: str):
    if complexity == "simple":
        return fast_model
    elif complexity == "complex":
        return smart_model
    else:
        return local_model
```

---

### 3. **Implement Proper Error Handling & Retries**

**Why:** API calls fail. Network issues happen. Rate limits exist.

```python
from langchain_core.exceptions import LLMException
from tenacity import retry, stop_after_attempt, wait_exponential
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4")

# LangChain built-in retry mechanism
@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
def call_llm_with_retry(prompt: str):
    try:
        response = llm.invoke(prompt)
        return response
    except Exception as e:
        print(f"Error calling LLM: {e}")
        raise

# Safe wrapper
def safe_llm_call(prompt: str, max_retries: int = 3) -> str:
    for attempt in range(max_retries):
        try:
            return llm.invoke(prompt).content
        except Exception as e:
            if attempt == max_retries - 1:
                print(f"Failed after {max_retries} attempts")
                raise
            wait_time = 2 ** attempt  # Exponential backoff
            print(f"Attempt {attempt + 1} failed, retrying in {wait_time}s...")
            import time
            time.sleep(wait_time)
```

---

### 4. **Monitor Token Usage and Costs**

**Why:** Tokens = money. GPT-4 is 10x more expensive than GPT-4o-mini.

```python
from langchain_openai import ChatOpenAI
from langchain_core.callbacks import get_openai_callback

llm = ChatOpenAI(model="gpt-4")

# Track token usage and costs
with get_openai_callback() as cb:
    response = llm.invoke("Explain quantum computing")

    print(f"Tokens used: {cb.total_tokens}")
    print(f"Prompt tokens: {cb.prompt_tokens}")
    print(f"Completion tokens: {cb.completion_tokens}")
    print(f"Total cost: ${cb.total_cost:.4f}")
```

**Cost Calculation Tips:**

```python
# Create a cost tracking system
class CostTracker:
    def __init__(self):
        self.total_cost = 0
        self.total_tokens = 0

    def log_call(self, model, prompt_tokens, completion_tokens):
        # Pricing example (update with current rates)
        pricing = {
            "gpt-4": {"prompt": 0.03, "completion": 0.06},
            "gpt-4o-mini": {"prompt": 0.00015, "completion": 0.0006},
        }

        rates = pricing.get(model, {})
        cost = (prompt_tokens * rates.get("prompt", 0) +
                completion_tokens * rates.get("completion", 0)) / 1000

        self.total_cost += cost
        self.total_tokens += prompt_tokens + completion_tokens
        return cost

tracker = CostTracker()
cost = tracker.log_call("gpt-4", 100, 50)
print(f"Call cost: ${cost:.4f}, Total: ${tracker.total_cost:.4f}")
```

---

### 5. **Use Temperature & Top-P Strategically**

**Why:** These parameters control randomness and directly impact output quality.

| Parameter       | Value   | Best For                     | Example                           |
| --------------- | ------- | ---------------------------- | --------------------------------- |
| **Temperature** | 0.0     | Deterministic, factual tasks | Data extraction, calculations     |
| **Temperature** | 0.3-0.7 | Balanced, general use        | QA, summarization                 |
| **Temperature** | 1.0+    | Creative, varied outputs     | Content generation, brainstorming |
| **Top-P**       | 0.1     | Focused, conservative        | Medical/legal advice              |
| **Top-P**       | 0.9     | Creative, diverse            | Creative writing                  |

```python
from langchain_openai import ChatOpenAI

# Deterministic: Same input = same output
extraction_model = ChatOpenAI(
    model="gpt-4",
    temperature=0.0,
    top_p=1.0
)

# Balanced: Good for most tasks
general_model = ChatOpenAI(
    model="gpt-4",
    temperature=0.7,
    top_p=0.9
)

# Creative: Varied and unpredictable
creative_model = ChatOpenAI(
    model="gpt-4",
    temperature=1.2,
    top_p=0.95
)

# Example: Task-specific model selection
def get_model_for_task(task_type: str):
    config = {
        "extraction": {"temperature": 0.0, "top_p": 1.0},
        "general": {"temperature": 0.7, "top_p": 0.9},
        "creative": {"temperature": 1.0, "top_p": 0.95},
    }
    settings = config.get(task_type, config["general"])
    return ChatOpenAI(model="gpt-4", **settings)
```

---

### 6. **Implement Context Window Management**

**Why:** Long conversations = higher costs + slower responses. Models have context limits.

```python
from langchain.memory import ConversationTokenBufferMemory
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4")

# Limit memory to ~4000 tokens (keep room for response)
memory = ConversationTokenBufferMemory(
    llm=llm,
    max_token_limit=4000,
    human_prefix="User",
    ai_prefix="Assistant"
)

# Add messages
memory.chat_memory.add_user_message("Hi, I'm learning LangChain")
memory.chat_memory.add_ai_message("Great! Let's start with basics.")
memory.chat_memory.add_user_message("What is LCEL?")

# Automatically summarizes old messages when hitting token limit
history = memory.load_memory_variables({})
print(history["history"])  # Only recent messages + summary
```

---

### 7. **Cache Embeddings & Responses**

**Why:** Duplicate embeddings waste money. Caching saves API calls.

```python
import hashlib
import json
from langchain_openai import OpenAIEmbeddings

class EmbeddingCache:
    def __init__(self):
        self.cache = {}

    def get_embedding(self, text: str, embeddings_model: OpenAIEmbeddings):
        # Create cache key
        cache_key = hashlib.md5(text.encode()).hexdigest()

        # Return cached embedding if exists
        if cache_key in self.cache:
            print(f"Cache hit for: {text[:30]}...")
            return self.cache[cache_key]

        # Fetch and cache embedding
        embedding = embeddings_model.embed_query(text)
        self.cache[cache_key] = embedding
        print(f"Cache miss, fetched for: {text[:30]}...")
        return embedding

    def save_cache(self, filepath: str):
        # Save cache to disk for persistence
        with open(filepath, 'w') as f:
            json.dump({k: v for k, v in self.cache.items()}, f)

    def load_cache(self, filepath: str):
        # Load previously cached embeddings
        try:
            with open(filepath, 'r') as f:
                self.cache = json.load(f)
        except FileNotFoundError:
            pass

# Usage
cache = EmbeddingCache()
embeddings = OpenAIEmbeddings()

# First call: fetches from API
vec1 = cache.get_embedding("machine learning", embeddings)

# Second call: uses cache
vec2 = cache.get_embedding("machine learning", embeddings)

# Persist cache for next run
cache.save_cache("embedding_cache.json")
```

---

### 8. **Use Structured Outputs (Modern Models)**

**Why:** Forces models to return valid JSON/Pydantic objects, eliminating parsing errors.

```python
from langchain_openai import ChatOpenAI
from pydantic import BaseModel, Field

class Person(BaseModel):
    name: str = Field(description="Person's full name")
    age: int = Field(description="Person's age")
    occupation: str = Field(description="What they do for work")

# Modern models support structured output
llm = ChatOpenAI(model="gpt-4-turbo")
structured_llm = llm.with_structured_output(Person)

result = structured_llm.invoke(
    "Extract info: John Smith is 35, works as a software engineer"
)

print(result)  # Person(name="John Smith", age=35, occupation="software engineer")
print(type(result))  # <class '__main__.Person'>
```

---

### 9. **Choose Right LLM Provider for Your Needs**

**Why:** Different providers have different strengths.

| Provider           | Strengths                    | Best For                  | Cost     |
| ------------------ | ---------------------------- | ------------------------- | -------- |
| **OpenAI**         | Reliable, fast, good models  | Production, general use   | Moderate |
| **Anthropic**      | Long context, good reasoning | Complex tasks, documents  | Moderate |
| **Google**         | Fast, multimodal             | Vision, real-time         | Low      |
| **Local (Ollama)** | Privacy, no API calls        | Development, private data | Free     |
| **Together AI**    | Cheap, open models           | Cost-sensitive, custom    | Very Low |

```python
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.llms import Ollama

# Multi-provider setup
def get_llm(provider: str, **kwargs):
    providers = {
        "openai": lambda: ChatOpenAI(model="gpt-4", **kwargs),
        "anthropic": lambda: ChatAnthropic(model="claude-3-sonnet-20240229", **kwargs),
        "google": lambda: ChatGoogleGenerativeAI(model="gemini-pro", **kwargs),
        "local": lambda: Ollama(model="mistral", **kwargs),
    }
    return providers.get(provider, providers["openai"])()

# Use in production to switch providers easily
production_llm = get_llm("openai")
dev_llm = get_llm("local")
```

---

### 10. **Implement Monitoring & Logging**

**Why:** Production systems fail silently. You need visibility.

```python
import logging
from langchain_core.callbacks import BaseCallbackHandler
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MonitoringCallback(BaseCallbackHandler):
    def on_llm_start(self, serialized, prompts, **kwargs):
        logger.info(f"LLM Call Started - Prompt: {prompts[0][:100]}...")
        self.start_time = datetime.now()

    def on_llm_end(self, response, **kwargs):
        duration = (datetime.now() - self.start_time).total_seconds()
        logger.info(f"LLM Call Completed in {duration:.2f}s")
        logger.info(f"Response: {response.generations[0][0].text[:100]}...")

    def on_llm_error(self, error: Exception, **kwargs):
        logger.error(f"LLM Error: {str(error)}")

# Usage
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4")
monitoring_callback = MonitoringCallback()

result = llm.invoke(
    "Hello",
    config={"callbacks": [monitoring_callback]}
)
```

---

## ğŸ¯ Part 2: Tips & Tricks for Models

### Trick 1: **Parallel Model Calls for Comparison**

Test multiple models simultaneously to see which performs best.

```python
import asyncio
from langchain_openai import ChatOpenAI

async def compare_models(prompt: str):
    models = [
        ChatOpenAI(model="gpt-4"),
        ChatOpenAI(model="gpt-4o-mini"),
    ]

    tasks = [model.ainvoke(prompt) for model in models]
    responses = await asyncio.gather(*tasks)

    for i, response in enumerate(responses):
        print(f"Model {i+1}: {response.content}")

# Run comparison
asyncio.run(compare_models("What is machine learning?"))
```

---

### Trick 2: **Model Fallback Chain**

If one model fails, automatically fallback to another.

```python
from langchain_openai import ChatOpenAI
from langchain_core.exceptions import LLMException

def call_with_fallback(prompt: str):
    primary = ChatOpenAI(model="gpt-4")
    fallback = ChatOpenAI(model="gpt-4o-mini")

    try:
        return primary.invoke(prompt)
    except LLMException as e:
        print(f"Primary model failed: {e}, using fallback")
        return fallback.invoke(prompt)

result = call_with_fallback("Explain quantum computing")
```

---

### Trick 3: **Dynamic Temperature Based on Task Complexity**

Automatically adjust temperature based on the task.

```python
from langchain_openai import ChatOpenAI

def get_model_for_complexity(complexity_score: float):
    """
    Complexity 0-1: Map to temperature 0-1.5
    0 = deterministic, 1 = balanced, 1.5 = creative
    """
    temperature = min(1.5, complexity_score * 1.5)
    return ChatOpenAI(model="gpt-4", temperature=temperature)

# Simple task = deterministic
simple_model = get_model_for_complexity(0.1)

# Complex task = creative
complex_model = get_model_for_complexity(0.8)
```

---

### Trick 4: **Batch Embed Multiple Texts Efficiently**

Embed many documents at once instead of one-by-one.

```python
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

documents = [
    "Machine learning is AI",
    "Deep learning uses neural networks",
    "NLP processes text",
]

# Batch embed (faster & cheaper than individual calls)
vectors = embeddings.embed_documents(documents)

# Single query
query_vector = embeddings.embed_query("What is machine learning?")

print(f"Embedded {len(vectors)} documents")
print(f"Vector dimension: {len(vectors[0])}")
```

---

### Trick 5: **Rate Limiting to Avoid API Throttling**

Control request rate to stay within API limits.

```python
import time
from typing import Callable
from functools import wraps
from langchain_openai import ChatOpenAI

def rate_limit(calls_per_minute: int):
    """Decorator to limit function calls per minute"""
    min_interval = 60 / calls_per_minute
    last_called = [0.0]

    def decorator(func: Callable):
        @wraps(func)
        def wrapper(*args, **kwargs):
            elapsed = time.time() - last_called[0]
            if elapsed < min_interval:
                time.sleep(min_interval - elapsed)

            result = func(*args, **kwargs)
            last_called[0] = time.time()
            return result
        return wrapper
    return decorator

@rate_limit(calls_per_minute=30)  # Max 30 calls/minute
def call_llm(prompt: str):
    llm = ChatOpenAI(model="gpt-4")
    return llm.invoke(prompt)

# This will automatically throttle to 30 calls/minute
for i in range(100):
    result = call_llm(f"Question {i}")
```

---

### Trick 6: **Use Model Aliases for Easy Switching**

Define model configurations and switch them easily.

```python
from langchain_openai import ChatOpenAI

MODEL_CONFIGS = {
    "fast": {
        "model": "gpt-4o-mini",
        "temperature": 0.7,
        "max_tokens": 500,
    },
    "smart": {
        "model": "gpt-4",
        "temperature": 0.5,
        "max_tokens": 2000,
    },
    "creative": {
        "model": "gpt-4",
        "temperature": 1.0,
        "max_tokens": 1500,
    },
}

def get_llm(alias: str = "fast"):
    config = MODEL_CONFIGS.get(alias, MODEL_CONFIGS["fast"])
    return ChatOpenAI(**config)

# Easy switching
llm = get_llm("smart")
result = llm.invoke("Complex reasoning task")
```

---

### Trick 7: **Stream Responses for Real-Time UX**

Show tokens as they arrive instead of waiting for full response.

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4", streaming=True)
prompt = "Write a short poem about AI"

print("Streaming response:")
for chunk in llm.stream(prompt):
    print(chunk.content, end="", flush=True)
```

---

### Trick 8: **Create Model Ensembles**

Combine multiple models to improve reliability and quality.

```python
from langchain_openai import ChatOpenAI
from typing import List

class ModelEnsemble:
    def __init__(self, models: List):
        self.models = models

    def invoke_parallel(self, prompt: str):
        """Get responses from all models"""
        responses = []
        for model in self.models:
            response = model.invoke(prompt).content
            responses.append(response)
        return responses

    def vote(self, prompt: str):
        """Use majority vote (for yes/no questions)"""
        responses = self.invoke_parallel(prompt)
        # Count yes/no answers
        yes_count = sum(1 for r in responses if "yes" in r.lower())
        return "yes" if yes_count > len(self.models) / 2 else "no"

# Create ensemble
ensemble = ModelEnsemble([
    ChatOpenAI(model="gpt-4"),
    ChatOpenAI(model="gpt-4"),
    ChatOpenAI(model="gpt-4o-mini"),
])

# Get multiple perspectives
responses = ensemble.invoke_parallel("Is climate change real?")
for i, response in enumerate(responses):
    print(f"Model {i+1}: {response[:100]}")

# Vote on answer
verdict = ensemble.vote("Should we invest in solar energy?")
print(f"Ensemble verdict: {verdict}")
```

---

### Trick 9: **Adaptive Model Selection Based on Input Length**

Use cheaper models for short inputs, expensive ones for complex tasks.

```python
from langchain_openai import ChatOpenAI

def select_model_by_input(prompt: str):
    """Choose model based on prompt complexity"""
    word_count = len(prompt.split())

    if word_count < 20:
        return ChatOpenAI(model="gpt-4o-mini")  # Cheap
    elif word_count < 100:
        return ChatOpenAI(model="gpt-4")  # Medium
    else:
        return ChatOpenAI(model="gpt-4-turbo")  # Expensive, handles long context

# Automatic model selection
prompt_short = "Hello"
prompt_long = "Please analyze this 2000-word document about quantum physics..."

model1 = select_model_by_input(prompt_short)  # Uses mini
model2 = select_model_by_input(prompt_long)   # Uses turbo
```

---

### Trick 10: **Create a Model Performance Benchmark**

Track which model performs best on your specific tasks.

```python
import time
import json
from langchain_openai import ChatOpenAI

class ModelBenchmark:
    def __init__(self):
        self.results = {}

    def benchmark(self, model_name: str, prompt: str, expected_keyword: str):
        """Test model on a prompt"""
        llm = ChatOpenAI(model=model_name)

        start = time.time()
        response = llm.invoke(prompt).content
        duration = time.time() - start

        # Score: 1 if contains keyword, 0 otherwise
        accuracy = 1 if expected_keyword.lower() in response.lower() else 0

        self.results[model_name] = {
            "accuracy": accuracy,
            "duration": duration,
            "cost_score": duration * 0.001,  # Example cost metric
        }

    def winner(self):
        """Find best model"""
        best = max(self.results.items(),
                  key=lambda x: x[1]["accuracy"] - x[1]["cost_score"])
        return best

# Run benchmark
benchmark = ModelBenchmark()
benchmark.benchmark("gpt-4", "What is LangChain?", "framework")
benchmark.benchmark("gpt-4o-mini", "What is LangChain?", "framework")

winner, scores = benchmark.winner()
print(f"Best model: {winner}")
print(f"Scores: {json.dumps(scores, indent=2)}")
```

---

### Trick 11: **Use System Messages Effectively**

Define model behavior with well-crafted system prompts.

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

# Create models with different system personalities
expert_prompt = ChatPromptTemplate.from_messages([
    ("system", """You are a senior software engineer with 20 years of experience.
    Provide expert-level advice. Be concise but thorough. Include code examples when relevant."""),
    ("user", "{question}")
])

beginner_prompt = ChatPromptTemplate.from_messages([
    ("system", """You are a friendly programming mentor for beginners.
    Explain concepts simply. Use analogies. Be encouraging."""),
    ("user", "{question}")
])

llm = ChatOpenAI(model="gpt-4")

expert_chain = expert_prompt | llm
beginner_chain = beginner_prompt | llm

# Same question, different expertise levels
question = "What are decorators in Python?"
print("Expert:", expert_chain.invoke({"question": question}).content)
print("\nBeginner:", beginner_chain.invoke({"question": question}).content)
```

---

### Trick 12: **Implement Smart Caching with TTL**

Cache responses with expiration time.

```python
import time
from typing import Optional

class SmartCache:
    def __init__(self, ttl_seconds: int = 3600):
        self.cache = {}
        self.ttl = ttl_seconds

    def get(self, key: str) -> Optional[str]:
        if key in self.cache:
            value, timestamp = self.cache[key]
            if time.time() - timestamp < self.ttl:
                return value
            else:
                del self.cache[key]  # Expired
        return None

    def set(self, key: str, value: str):
        self.cache[key] = (value, time.time())

    def clear_expired(self):
        now = time.time()
        expired = [k for k, (v, t) in self.cache.items() if now - t > self.ttl]
        for k in expired:
            del self.cache[k]

# Usage
cache = SmartCache(ttl_seconds=300)  # 5 minute TTL

# Store response
cache.set("question_1", "Response about LangChain...")

# Retrieve (valid for 5 minutes)
response = cache.get("question_1")

# After 5 minutes, returns None
time.sleep(301)
response = cache.get("question_1")  # None
```

---

## ğŸ“Š Quick Reference: Model Selection Matrix

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Task Type           â”‚ Recommended    â”‚ Temperature â”‚ Max Tokens   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Data Extraction     â”‚ gpt-4o-mini    â”‚ 0.0         â”‚ 500          â”‚
â”‚ QA                  â”‚ gpt-4          â”‚ 0.7         â”‚ 1000         â”‚
â”‚ Summarization       â”‚ gpt-4          â”‚ 0.5         â”‚ 1000         â”‚
â”‚ Code Generation     â”‚ gpt-4          â”‚ 0.7         â”‚ 2000         â”‚
â”‚ Creative Writing    â”‚ gpt-4          â”‚ 1.0         â”‚ 2000         â”‚
â”‚ Translation         â”‚ gpt-4o-mini    â”‚ 0.3         â”‚ 1000         â”‚
â”‚ Semantic Search     â”‚ text-embedding â”‚ N/A         â”‚ N/A          â”‚
â”‚ Complex Reasoning   â”‚ gpt-4 turbo    â”‚ 0.5         â”‚ 4000         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Production Checklist

- [ ] API keys in environment variables, not hardcoded
- [ ] Error handling with exponential backoff retries
- [ ] Token usage monitoring with cost tracking
- [ ] Appropriate temperature/top_p for your use case
- [ ] Token buffer memory to manage context windows
- [ ] Embedding cache to reduce API calls
- [ ] Logging and monitoring for visibility
- [ ] Rate limiting to respect API quotas
- [ ] Fallback models for reliability
- [ ] Structured outputs (Pydantic validation)
- [ ] Model versioning for reproducibility
- [ ] Load testing before production deployment

---

**Keep learning. Keep optimizing. Build better LLM applications.** ğŸš€

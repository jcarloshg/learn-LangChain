# â›“ï¸ Best Practices & Tips for Managing Chains in LangChain

> LCEL, Legacy Chains, and Advanced Workflows Mastery Guide

---

## ðŸ“‘ Table of Contents

### Part 1: Best Practices for Chain Management
1. [Use LCEL Over Legacy Chains](#1-use-lcel-over-legacy-chains)
2. [Compose Chains Using the Pipe Operator](#2-compose-chains-using-the-pipe-operator)
3. [Handle Different Input/Output Types Correctly](#3-handle-different-inputoutput-types-correctly)
4. [Use Runnable Methods for Chain Control](#4-use-runnable-methods-for-chain-control)
5. [Implement Proper Error Handling in Chains](#5-implement-proper-error-handling-in-chains)
6. [Build RAG Chains Properly](#6-build-rag-chains-properly)
7. [Use Conditional Logic in Chains](#7-use-conditional-logic-in-chains)
8. [Implement Chain Caching and Memoization](#8-implement-chain-caching-and-memoization)
9. [Debug Chains Effectively](#9-debug-chains-effectively)
10. [Parallelize Chain Execution](#10-parallelize-chain-execution)

### Part 2: Tips & Tricks for Chains
1. [Chain Composition Patterns](#trick-1-chain-composition-patterns)
2. [Dynamic Chain Selection](#trick-2-dynamic-chain-selection)
3. [Chain Retries with Exponential Backoff](#trick-3-chain-retries-with-exponential-backoff)
4. [Chain Profiling and Performance Monitoring](#trick-4-chain-profiling-and-performance-monitoring)
5. [Chain Composition with Branching](#trick-5-chain-composition-with-branching)
6. [Chain Composition with State](#trick-6-chain-composition-with-state)
7. [Chain Composition with Filtering](#trick-7-chain-composition-with-filtering)
8. [Chain Composition with Logging](#trick-8-chain-composition-with-logging)
9. [Chain Composition with Transform Functions](#trick-9-chain-composition-with-transform-functions)
10. [Chain Composition with Mapping](#trick-10-chain-composition-with-mapping)
11. [Chain Composition with Validation](#trick-11-chain-composition-with-validation)
12. [Legacy Chain Migration Guide](#trick-12-legacy-chain-migration-guide)

### Reference & Checklists
- [Quick Reference: Chain Patterns](#-quick-reference-chain-patterns)
- [Chain Execution Methods Quick Reference](#-chain-execution-methods-quick-reference)
- [Production Chain Checklist](#-production-chain-checklist)

---

## ðŸ“‹ Part 1: Best Practices for Chain Management

### 1. **Use LCEL Over Legacy Chains**

**Why:** LCEL (LangChain Expression Language) is the modern, composable, and type-safe way to build chains. Legacy chains are deprecated.

```python
# âŒ Old/Legacy approach (Deprecated)
from langchain.chains import LLMChain
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate

prompt = PromptTemplate.from_template("Explain {topic}")
llm = ChatOpenAI(model="gpt-4")
chain = LLMChain(llm=llm, prompt=prompt)
result = chain.run(topic="AI")

# âœ… Modern LCEL approach (Recommended)
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

prompt = ChatPromptTemplate.from_template("Explain {topic}")
llm = ChatOpenAI(model="gpt-4")
chain = prompt | llm
result = chain.invoke({"topic": "AI"})

# LCEL Benefits:
# 1. Cleaner, more readable code
# 2. Full type safety
# 3. Better debugging
# 4. Streaming support built-in
# 5. Parallel execution
# 6. Fallback handling
```

**Why LCEL is Better:**

| Aspect             | Legacy    | LCEL                  |
| ------------------ | --------- | --------------------- |
| **Syntax**         | Verbose   | Clean (`\|` operator) |
| **Composability**  | Limited   | Excellent             |
| **Type Safety**    | Weak      | Strong                |
| **Streaming**      | Manual    | Built-in              |
| **Debugging**      | Difficult | Easy                  |
| **Performance**    | Slower    | Faster                |
| **Learning curve** | Steep     | Gentle                |

---

### 2. **Compose Chains Using the Pipe Operator**

**Why:** The `|` operator makes chains readable, maintainable, and composable.

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

# Single pipes: Simple chain
prompt = ChatPromptTemplate.from_template("Explain {topic}")
llm = ChatOpenAI(model="gpt-4")
parser = StrOutputParser()

simple_chain = prompt | llm | parser
result = simple_chain.invoke({"topic": "machine learning"})

# Multiple pipes: Complex chain
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.output_parsers import JsonOutputParser

complex_chain = (
    prompt
    | llm
    | parser
    | (lambda x: x.split('\n'))  # Split output
    | (lambda lines: '\n'.join(lines[:5]))  # Take first 5 lines
)

# Pipes are left-to-right: intuitive and readable
# prompt â†’ llm â†’ parser â†’ split â†’ limit
```

**Best Practices for Composing:**

```python
# âŒ Avoid: Overly complex single chain
bad_chain = prompt1 | llm1 | transform1 | llm2 | transform2 | parser | transform3

# âœ… Good: Break into logical steps
analysis_chain = prompt1 | llm1 | parser
extraction_chain = analysis_chain | extract_function
refinement_chain = extraction_chain | refine_function

# âœ… Even better: Name each step
def analyze_document(doc):
    analysis = analysis_chain.invoke({"doc": doc})
    extraction = extraction_chain.invoke({"analysis": analysis})
    refined = refinement_chain.invoke({"extraction": extraction})
    return refined
```

---

### 3. **Handle Different Input/Output Types Correctly**

**Why:** Type mismatches cause silent failures. Be explicit about what goes in and comes out.

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from typing import List, Dict

# âœ… Good: Clear input/output types
prompt = ChatPromptTemplate.from_template(
    "Summarize: {text}"
)  # Expects: {"text": str}

llm = ChatOpenAI(model="gpt-4")  # Outputs: BaseMessage

parser = (
    lambda x: x.content  # Convert BaseMessage â†’ str
)

chain = prompt | llm | parser  # Final output: str

# âœ… Type annotations for clarity
def process_documents(docs: List[str]) -> List[str]:
    """Process multiple documents"""
    results = []
    for doc in docs:
        result = chain.invoke({"text": doc})
        results.append(result)
    return results

# âœ… Dictionary output for complex data
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field

class Summary(BaseModel):
    title: str = Field(description="Document title")
    summary: str = Field(description="Brief summary")
    keywords: List[str] = Field(description="Key topics")

dict_chain = (
    prompt
    | llm
    | JsonOutputParser(pydantic_object=Summary)
)  # Output: Summary object

# Usage
result = dict_chain.invoke({"text": "Your document..."})
print(result.title)  # Type-safe access
```

---

### 4. **Use Runnable Methods for Chain Control**

**Why:** LCEL runnables have powerful methods for streaming, async, and batch operations.

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
import asyncio

prompt = ChatPromptTemplate.from_template("Explain {topic}")
llm = ChatOpenAI(model="gpt-4")
chain = prompt | llm

# 1. invoke() - Synchronous, single call
result = chain.invoke({"topic": "AI"})

# 2. stream() - Stream tokens as they arrive
print("Streaming response:")
for chunk in chain.stream({"topic": "AI"}):
    print(chunk.content, end="", flush=True)

# 3. batch() - Process multiple inputs efficiently
inputs = [
    {"topic": "AI"},
    {"topic": "ML"},
    {"topic": "DL"}
]
results = chain.batch(inputs)

# 4. ainvoke() - Async single call
async def async_call():
    result = await chain.ainvoke({"topic": "AI"})
    return result

result = asyncio.run(async_call())

# 5. astream() - Async streaming
async def async_stream():
    async for chunk in chain.astream({"topic": "AI"}):
        print(chunk.content, end="", flush=True)

asyncio.run(async_stream())

# 6. abatch() - Async batch
async def async_batch():
    results = await chain.abatch(inputs)
    return results

results = asyncio.run(async_batch())

# Choose the right method for your use case
class ChainExecutor:
    def __init__(self, chain):
        self.chain = chain

    def execute_single(self, input_dict):
        """For single queries"""
        return self.chain.invoke(input_dict)

    def execute_streaming(self, input_dict):
        """For real-time UI updates"""
        for chunk in self.chain.stream(input_dict):
            yield chunk

    def execute_batch(self, inputs):
        """For processing many at once"""
        return self.chain.batch(inputs)

    async def execute_async(self, input_dict):
        """For concurrent operations"""
        return await self.chain.ainvoke(input_dict)
```

---

### 5. **Implement Proper Error Handling in Chains**

**Why:** Chains fail. Network errors, rate limits, invalid inputs. Handle gracefully.

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import JsonOutputParser
from tenacity import retry, stop_after_attempt, wait_exponential

prompt = ChatPromptTemplate.from_template("Analyze: {text}")
llm = ChatOpenAI(model="gpt-4")
chain = prompt | llm

# âœ… Basic error handling
def safe_chain_call(text: str, max_retries: int = 3):
    for attempt in range(max_retries):
        try:
            result = chain.invoke({"text": text})
            return result.content
        except Exception as e:
            print(f"Attempt {attempt + 1} failed: {e}")
            if attempt == max_retries - 1:
                return "Error: Could not process request"

            # Exponential backoff
            wait_time = 2 ** attempt
            import time
            time.sleep(wait_time)

# âœ… Advanced error handling with fallback
def chain_with_fallback(text: str):
    primary_chain = prompt | llm | JsonOutputParser()
    fallback_chain = prompt | llm  # No parser

    try:
        # Try primary
        return primary_chain.invoke({"text": text})
    except Exception as e:
        print(f"Primary failed: {e}, using fallback")
        try:
            return fallback_chain.invoke({"text": text})
        except Exception as fallback_error:
            print(f"Fallback also failed: {fallback_error}")
            return None

# âœ… Chain with validation
def validated_chain(text: str, min_length: int = 10):
    # Validate input
    if not isinstance(text, str):
        raise ValueError("Input must be string")
    if len(text) < min_length:
        raise ValueError(f"Input must be at least {min_length} chars")

    # Execute chain
    try:
        result = chain.invoke({"text": text})

        # Validate output
        if not result or not result.content:
            raise ValueError("Empty response")

        return result.content

    except ValueError as ve:
        print(f"Validation error: {ve}")
        raise
    except Exception as e:
        print(f"Execution error: {e}")
        raise

# âœ… Chain with timeout
import asyncio
from contextlib import asynccontextmanager

async def chain_with_timeout(text: str, timeout_seconds: int = 10):
    try:
        result = await asyncio.wait_for(
            chain.ainvoke({"text": text}),
            timeout=timeout_seconds
        )
        return result
    except asyncio.TimeoutError:
        print(f"Chain timed out after {timeout_seconds}s")
        return None
    except Exception as e:
        print(f"Chain error: {e}")
        return None
```

---

### 6. **Build RAG Chains Properly**

**Why:** RAG (Retrieval-Augmented Generation) is a common pattern. Structure matters.

```python
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from operator import itemgetter

# Step 1: Load and prepare documents
loader = TextLoader("document.txt")
docs = loader.load()
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = splitter.split_documents(docs)

# Step 2: Create vector store
embeddings = OpenAIEmbeddings()
vector_store = FAISS.from_documents(chunks, embeddings)
retriever = vector_store.as_retriever(search_kwargs={"k": 3})

# Step 3: Build RAG chain
template = ChatPromptTemplate.from_template(
    """Use the following context to answer the question.

Context:
{context}

Question: {question}

Answer:"""
)

llm = ChatOpenAI(model="gpt-4")
parser = StrOutputParser()

# âœ… Proper RAG chain structure
rag_chain = (
    {
        "context": retriever | (lambda docs: "\n".join([d.page_content for d in docs])),
        "question": itemgetter("question")
    }
    | template
    | llm
    | parser
)

# Usage
result = rag_chain.invoke({"question": "What is the main topic?"})

# âœ… RAG chain with metadata
def rag_with_metadata():
    """RAG that also returns source documents"""

    retrieval_chain = (
        {
            "context": retriever,
            "question": itemgetter("question")
        }
        | template
        | llm
    )

    # Also get source docs
    sources = retriever.invoke("main topic")

    return {
        "answer": result,
        "sources": sources
    }

# âœ… Multi-retriever RAG
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever

# Combine dense and sparse retrieval
bm25_retriever = BM25Retriever.from_documents(chunks)
dense_retriever = vector_store.as_retriever()

ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, dense_retriever],
    weights=[0.5, 0.5]
)

multi_rag_chain = (
    {
        "context": ensemble_retriever | (lambda docs: "\n".join([d.page_content for d in docs])),
        "question": itemgetter("question")
    }
    | template
    | llm
    | parser
)
```

---

### 7. **Use Conditional Logic in Chains**

**Why:** Real workflows branch. Use conditionals to route logic.

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4")

# âœ… Conditional routing with RunnableBranch
from langchain_core.runnables import RunnableBranch

def classify_query(query: str) -> str:
    """Simple classifier for routing"""
    if "code" in query.lower():
        return "technical"
    elif "story" in query.lower() or "creative" in query.lower():
        return "creative"
    else:
        return "general"

# Define chains for each branch
technical_prompt = ChatPromptTemplate.from_template(
    "You are a technical expert. Answer: {query}"
)
creative_prompt = ChatPromptTemplate.from_template(
    "You are a creative writer. Answer: {query}"
)
general_prompt = ChatPromptTemplate.from_template(
    "You are a helpful assistant. Answer: {query}"
)

technical_chain = technical_prompt | llm
creative_chain = creative_prompt | llm
general_chain = general_prompt | llm

# Use RunnableBranch for routing
branched_chain = RunnableBranch(
    (
        lambda x: "code" in x["query"].lower(),
        technical_chain
    ),
    (
        lambda x: "creative" in x["query"].lower() or "story" in x["query"].lower(),
        creative_chain
    ),
    general_chain  # Default
)

# Usage
result = branched_chain.invoke({"query": "How do I write Python code?"})

# âœ… Alternative: Custom conditional logic
def conditional_chain(query: str):
    """Custom routing logic"""

    category = classify_query(query)

    chains = {
        "technical": technical_chain,
        "creative": creative_chain,
        "general": general_chain
    }

    selected_chain = chains.get(category, general_chain)
    return selected_chain.invoke({"query": query})
```

---

### 8. **Implement Chain Caching and Memoization**

**Why:** Repeated calls with same input = wasted API calls. Cache results.

```python
import hashlib
import json
from functools import lru_cache
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

prompt = ChatPromptTemplate.from_template("Explain {topic}")
llm = ChatOpenAI(model="gpt-4")
chain = prompt | llm

# âœ… Simple cache decorator
class ChainCache:
    def __init__(self):
        self.cache = {}

    def _get_cache_key(self, input_dict: dict) -> str:
        """Create deterministic cache key"""
        content = json.dumps(input_dict, sort_keys=True)
        return hashlib.md5(content.encode()).hexdigest()

    def invoke(self, chain, input_dict: dict):
        """Call chain with caching"""
        cache_key = self._get_cache_key(input_dict)

        if cache_key in self.cache:
            print(f"Cache hit for {input_dict}")
            return self.cache[cache_key]

        print(f"Cache miss, executing chain")
        result = chain.invoke(input_dict)
        self.cache[cache_key] = result

        return result

    def clear(self):
        """Clear cache"""
        self.cache.clear()

# Usage
cache = ChainCache()
result1 = cache.invoke(chain, {"topic": "AI"})
result2 = cache.invoke(chain, {"topic": "AI"})  # Uses cache

# âœ… LRU cache with Python's functools
@lru_cache(maxsize=128)
def cached_chain_call(topic: str):
    """Automatically cached last 128 unique calls"""
    # Note: Must return hashable type
    result = chain.invoke({"topic": topic})
    return result.content

# âœ… TTL-based cache (expires after time)
import time

class TTLCache:
    def __init__(self, ttl_seconds: int = 3600):
        self.cache = {}
        self.ttl = ttl_seconds

    def get(self, key: str):
        if key in self.cache:
            value, timestamp = self.cache[key]
            if time.time() - timestamp < self.ttl:
                return value
            else:
                del self.cache[key]
        return None

    def set(self, key: str, value):
        self.cache[key] = (value, time.time())

    def invoke(self, chain, input_dict: dict):
        key = json.dumps(input_dict, sort_keys=True)

        cached = self.get(key)
        if cached:
            return cached

        result = chain.invoke(input_dict)
        self.set(key, result)
        return result

# Usage
ttl_cache = TTLCache(ttl_seconds=300)
result = ttl_cache.invoke(chain, {"topic": "AI"})
```

---

### 9. **Debug Chains Effectively**

**Why:** When chains break, you need visibility into what's happening.

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.callbacks import StreamingStdOutCallbackHandler

prompt = ChatPromptTemplate.from_template("Explain {topic}")
llm = ChatOpenAI(model="gpt-4")
chain = prompt | llm

# âœ… Debug with callbacks
class DebugCallback:
    def on_llm_start(self, serialized, prompts, **kwargs):
        print(f"\n=== LLM START ===")
        print(f"Prompt: {prompts[0][:200]}...")

    def on_llm_end(self, response, **kwargs):
        print(f"\n=== LLM END ===")
        print(f"Response: {response.generations[0][0].text[:200]}...")

    def on_llm_error(self, error: Exception, **kwargs):
        print(f"\n=== LLM ERROR ===")
        print(f"Error: {str(error)}")

# Use debug callback
debug_callback = DebugCallback()
result = chain.invoke(
    {"topic": "AI"},
    config={"callbacks": [debug_callback]}
)

# âœ… Verbose debugging
def debug_chain(chain, input_dict):
    """Execute chain with detailed logging"""
    print(f"\nInput: {input_dict}")
    print(f"Chain: {chain}")

    result = chain.invoke(input_dict)

    print(f"Output: {result}")
    print(f"Output type: {type(result)}")

    return result

# âœ… Step-by-step debugging
class DebuggableChain:
    def __init__(self, chain):
        self.chain = chain
        self.debug = False

    def invoke(self, input_dict, debug=False):
        if debug:
            print(f"Step 1: Input = {input_dict}")

        result = self.chain.invoke(input_dict)

        if debug:
            print(f"Step 2: Output = {result}")

        return result

# âœ… LangSmith for production debugging
# Set environment variable: LANGCHAIN_TRACING_V2=true
# This logs all chain executions to LangSmith dashboard
import os
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "your-api-key"

# All subsequent chains will be traced
result = chain.invoke({"topic": "AI"})
# View execution in https://smith.langchain.com
```

---

### 10. **Parallelize Chain Execution**

**Why:** Multiple independent chains can run concurrently. Faster results.

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableParallel
import asyncio

llm = ChatOpenAI(model="gpt-4")

# Define parallel chains
summarize_chain = ChatPromptTemplate.from_template(
    "Summarize: {text}"
) | llm

translate_chain = ChatPromptTemplate.from_template(
    "Translate to Spanish: {text}"
) | llm

sentiment_chain = ChatPromptTemplate.from_template(
    "Analyze sentiment: {text}"
) | llm

# âœ… Run chains in parallel
parallel_chain = RunnableParallel(
    summary=summarize_chain,
    translation=translate_chain,
    sentiment=sentiment_chain
)

# All three run concurrently
results = parallel_chain.invoke({
    "text": "Your document here..."
})

print(results["summary"].content)
print(results["translation"].content)
print(results["sentiment"].content)

# âœ… Async parallel execution
async def parallel_async():
    results = await parallel_chain.ainvoke({
        "text": "Your document here..."
    })
    return results

results = asyncio.run(parallel_async())
```

---

## ðŸŽ¯ Part 2: Tips & Tricks for Chains

### Trick 1: **Chain Composition Patterns**

Create reusable chain patterns for common tasks.

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

llm = ChatOpenAI(model="gpt-4")

# Pattern 1: Map-Filter-Reduce
def map_filter_reduce_chain(texts: list, transform_prompt: str, filter_fn, reduce_fn):
    """
    Map: Apply transformation to each text
    Filter: Keep only matching results
    Reduce: Combine results
    """

    prompt = ChatPromptTemplate.from_template(transform_prompt)
    chain = prompt | llm | StrOutputParser()

    # Map
    transformed = [chain.invoke({"text": t}) for t in texts]

    # Filter
    filtered = [t for t in transformed if filter_fn(t)]

    # Reduce
    return reduce_fn(filtered)

# Usage
results = map_filter_reduce_chain(
    texts=["doc1", "doc2", "doc3"],
    transform_prompt="Summarize: {text}",
    filter_fn=lambda x: len(x) > 50,
    reduce_fn=lambda summaries: "\n\n".join(summaries)
)

# Pattern 2: Pipeline
class Pipeline:
    def __init__(self):
        self.steps = []

    def add_step(self, name: str, chain):
        self.steps.append((name, chain))
        return self

    def execute(self, input_data):
        """Execute steps sequentially"""
        result = input_data
        for name, chain in self.steps:
            print(f"Executing: {name}")
            result = chain.invoke(result)
        return result

# Usage
pipeline = Pipeline()
pipeline.add_step("summarize", summarize_chain)
pipeline.add_step("translate", translate_chain)
pipeline.add_step("sentiment", sentiment_chain)

result = pipeline.execute({"text": "Your document..."})

# Pattern 3: Fan-out/Fan-in
def fan_out_fan_in(input_data, chains: dict):
    """Execute multiple chains, combine results"""
    from langchain_core.runnables import RunnableParallel

    parallel = RunnableParallel(**chains)
    return parallel.invoke(input_data)

# Usage
results = fan_out_fan_in(
    {"text": "data"},
    {
        "summary": summarize_chain,
        "translation": translate_chain,
        "sentiment": sentiment_chain
    }
)
```

---

### Trick 2: **Dynamic Chain Selection**

Choose which chain to execute based on input.

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4")

# Define specialized chains
code_chain = ChatPromptTemplate.from_template(
    "Review this code for bugs: {code}"
) | llm

doc_chain = ChatPromptTemplate.from_template(
    "Summarize this document: {doc}"
) | llm

question_chain = ChatPromptTemplate.from_template(
    "Answer this question: {question}"
) | llm

class DynamicChainSelector:
    def __init__(self):
        self.chains = {
            "code": code_chain,
            "document": doc_chain,
            "question": question_chain
        }

    def detect_type(self, input_dict) -> str:
        """Detect input type"""
        if "code" in input_dict:
            return "code"
        elif "doc" in input_dict:
            return "document"
        else:
            return "question"

    def invoke(self, input_dict):
        input_type = self.detect_type(input_dict)
        chain = self.chains[input_type]
        return chain.invoke(input_dict)

# Usage
selector = DynamicChainSelector()

result1 = selector.invoke({"code": "def foo(): pass"})
result2 = selector.invoke({"doc": "The report..."})
result3 = selector.invoke({"question": "How to learn?"})
```

---

### Trick 3: **Chain Retries with Exponential Backoff**

Automatically retry failed chains.

```python
import time
from functools import wraps
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4")
chain = ChatPromptTemplate.from_template("Explain {topic}") | llm

def retry_chain(max_retries: int = 3, base_wait: int = 2):
    """Decorator for automatic retries"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_retries - 1:
                        raise

                    wait_time = base_wait ** attempt
                    print(f"Attempt {attempt + 1} failed, retrying in {wait_time}s...")
                    time.sleep(wait_time)

        return wrapper
    return decorator

@retry_chain(max_retries=3)
def call_chain(topic: str):
    return chain.invoke({"topic": topic})

result = call_chain("machine learning")
```

---

### Trick 4: **Chain Profiling and Performance Monitoring**

Track chain execution time and performance.

```python
import time
from typing import Any, Dict
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4")
chain = ChatPromptTemplate.from_template("Explain {topic}") | llm

class ChainProfiler:
    def __init__(self):
        self.metrics = []

    def profile(self, chain, input_dict: dict) -> Dict[str, Any]:
        """Execute chain and capture metrics"""
        start = time.time()

        result = chain.invoke(input_dict)

        duration = time.time() - start

        metric = {
            "input": input_dict,
            "duration": duration,
            "tokens_in": len(str(input_dict).split()),
            "tokens_out": len(str(result).split()),
            "timestamp": time.time()
        }

        self.metrics.append(metric)
        return result, metric

    def get_stats(self):
        """Get performance statistics"""
        if not self.metrics:
            return None

        durations = [m["duration"] for m in self.metrics]

        return {
            "total_calls": len(self.metrics),
            "avg_duration": sum(durations) / len(durations),
            "min_duration": min(durations),
            "max_duration": max(durations),
            "total_duration": sum(durations)
        }

# Usage
profiler = ChainProfiler()

for topic in ["AI", "ML", "DL"]:
    result, metrics = profiler.profile(chain, {"topic": topic})
    print(f"Call took {metrics['duration']:.2f}s")

stats = profiler.get_stats()
print(f"Average duration: {stats['avg_duration']:.2f}s")
```

---

### Trick 5: **Chain Composition with Branching**

Build chains that branch based on conditions.

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableBranch

llm = ChatOpenAI(model="gpt-4")

# Define branch chains
positive_prompt = ChatPromptTemplate.from_template(
    "Generate a positive response to: {input}"
)

negative_prompt = ChatPromptTemplate.from_template(
    "Generate a critical response to: {input}"
)

neutral_prompt = ChatPromptTemplate.from_template(
    "Generate a neutral response to: {input}"
)

positive_chain = positive_prompt | llm
negative_chain = negative_prompt | llm
neutral_chain = neutral_prompt | llm

# Create branching chain
branching_chain = RunnableBranch(
    (
        lambda x: x.get("tone") == "positive",
        positive_chain
    ),
    (
        lambda x: x.get("tone") == "negative",
        negative_chain
    ),
    neutral_chain  # Default
)

# Usage
result = branching_chain.invoke({
    "input": "climate change",
    "tone": "positive"
})
```

---

### Trick 6: **Chain Composition with State**

Maintain state across chain executions.

```python
from typing import Any, Dict
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4")

class StatefulChain:
    def __init__(self, chain):
        self.chain = chain
        self.state = {}

    def invoke(self, input_dict: dict, state_key: str = None):
        """Execute chain and optionally store state"""
        result = self.chain.invoke(input_dict)

        if state_key:
            self.state[state_key] = result

        return result

    def get_state(self, key: str = None):
        """Retrieve state"""
        if key:
            return self.state.get(key)
        return self.state

    def clear_state(self):
        """Clear state"""
        self.state = {}

# Usage
prompt = ChatPromptTemplate.from_template("Explain {topic}")
chain = prompt | llm

stateful = StatefulChain(chain)

# Store intermediate results
analysis = stateful.invoke({"topic": "AI"}, state_key="analysis")
summary = stateful.invoke({"topic": "AI classification"}, state_key="summary")

# Access previous results
print(stateful.get_state("analysis"))
```

---

### Trick 7: **Chain Composition with Filtering**

Filter outputs from chains.

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4")
chain = ChatPromptTemplate.from_template("Generate ideas: {topic}") | llm

def filter_output(output, min_length: int = 100):
    """Filter chain output by length"""
    content = output.content
    return content if len(content) > min_length else None

class FilteredChain:
    def __init__(self, chain, filter_fn):
        self.chain = chain
        self.filter_fn = filter_fn

    def invoke(self, input_dict):
        output = self.chain.invoke(input_dict)
        filtered = self.filter_fn(output)

        if filtered is None:
            # Retry with different input
            print("Output filtered out, retrying...")
            return self.invoke(input_dict)

        return filtered

# Usage
filtered_chain = FilteredChain(chain, lambda x: filter_output(x, min_length=200))
result = filtered_chain.invoke({"topic": "Machine Learning"})
```

---

### Trick 8: **Chain Composition with Logging**

Add logging at each chain step.

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
import logging
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

llm = ChatOpenAI(model="gpt-4")

class LoggedChain:
    def __init__(self, chain, chain_name: str = "Chain"):
        self.chain = chain
        self.name = chain_name

    def invoke(self, input_dict):
        logger.info(f"[{self.name}] Starting execution at {datetime.now()}")
        logger.debug(f"[{self.name}] Input: {input_dict}")

        try:
            result = self.chain.invoke(input_dict)
            logger.info(f"[{self.name}] Completed successfully")
            logger.debug(f"[{self.name}] Output length: {len(str(result))}")
            return result

        except Exception as e:
            logger.error(f"[{self.name}] Failed with error: {str(e)}")
            raise

# Usage
prompt = ChatPromptTemplate.from_template("Explain {topic}")
chain = LoggedChain(prompt | llm, "ExplanationChain")

result = chain.invoke({"topic": "LCEL"})
```

---

### Trick 9: **Chain Composition with Transform Functions**

Transform data between chain steps.

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4")

# Define transformation functions
def extract_first_sentence(text: str) -> str:
    """Extract first sentence"""
    sentences = text.split('.')
    return sentences[0] + '.' if sentences else text

def count_words(text: str) -> int:
    """Count words"""
    return len(text.split())

def uppercase(text: str) -> str:
    """Convert to uppercase"""
    return text.upper()

# Build chain with transformations
chain = (
    ChatPromptTemplate.from_template("Generate ideas: {topic}")
    | llm
    | (lambda x: x.content)  # Extract content
    | extract_first_sentence  # Transform 1
    | uppercase  # Transform 2
)

# Usage
result = chain.invoke({"topic": "AI"})
```

---

### Trick 10: **Chain Composition with Mapping**

Map chains over collections of inputs.

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from typing import List

llm = ChatOpenAI(model="gpt-4")
chain = ChatPromptTemplate.from_template("Summarize: {text}") | llm

class ChainMapper:
    def __init__(self, chain):
        self.chain = chain

    def map(self, inputs: List[dict]) -> List:
        """Map chain over multiple inputs"""
        return [self.chain.invoke(inp) for inp in inputs]

    def map_batch(self, inputs: List[dict]) -> List:
        """Batch process inputs (more efficient)"""
        return self.chain.batch(inputs)

    def map_async(self, inputs: List[dict]):
        """Async map over inputs"""
        import asyncio
        return asyncio.run(self.chain.abatch(inputs))

# Usage
mapper = ChainMapper(chain)

documents = [
    {"text": "Doc 1..."},
    {"text": "Doc 2..."},
    {"text": "Doc 3..."}
]

summaries = mapper.map_batch(documents)
```

---

### Trick 11: **Chain Composition with Validation**

Validate inputs and outputs at each step.

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from pydantic import BaseModel, validator, ValidationError

llm = ChatOpenAI(model="gpt-4")

# Define validation schemas
class QueryInput(BaseModel):
    topic: str

    @validator('topic')
    def topic_not_empty(cls, v):
        if not v or len(v) < 3:
            raise ValueError('Topic must be at least 3 characters')
        return v

class Response(BaseModel):
    content: str

    @validator('content')
    def content_not_empty(cls, v):
        if not v or len(v) < 50:
            raise ValueError('Response too short')
        return v

class ValidatedChain:
    def __init__(self, chain):
        self.chain = chain

    def invoke(self, input_dict: dict):
        # Validate input
        try:
            validated_input = QueryInput(**input_dict)
        except ValidationError as e:
            print(f"Input validation failed: {e}")
            raise

        # Execute chain
        result = self.chain.invoke(input_dict)

        # Validate output
        try:
            validated_output = Response(content=result.content)
        except ValidationError as e:
            print(f"Output validation failed: {e}")
            raise

        return validated_output

# Usage
prompt = ChatPromptTemplate.from_template("Explain {topic}")
chain = ValidatedChain(prompt | llm)

result = chain.invoke({"topic": "Machine Learning"})
```

---

### Trick 12: **Legacy Chain Migration Guide**

Migrate from legacy chains to LCEL.

```python
# âŒ Legacy approach
from langchain.chains import LLMChain, SequentialChain
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate

llm = ChatOpenAI(model="gpt-4")

# Legacy: Individual chains
prompt1 = PromptTemplate.from_template("Summarize: {text}")
chain1 = LLMChain(llm=llm, prompt=prompt1)

prompt2 = PromptTemplate.from_template("Translate to Spanish: {text}")
chain2 = LLMChain(llm=llm, prompt=prompt2)

# Legacy: Combine chains
legacy_sequential = SequentialChain(
    chains=[chain1, chain2],
    input_variables=["text"]
)

# âœ… Modern LCEL approach
from langchain_core.prompts import ChatPromptTemplate

summarize_prompt = ChatPromptTemplate.from_template("Summarize: {text}")
translate_prompt = ChatPromptTemplate.from_template("Translate to Spanish: {text}")

# Modern: Compose with pipe operator
lcel_chain = (
    summarize_prompt
    | llm
    | (lambda x: x.content)  # Extract content
    | (lambda summary: {"text": summary})  # Format for next step
    | translate_prompt
    | llm
)

# âœ… Migration checklist
"""
Legacy â†’ Modern migration steps:

1. Replace LLMChain with: prompt | llm
2. Replace SequentialChain with: chain1 | chain2 | chain3
3. Replace chains.run() with: chain.invoke()
4. Add explicit output parsing (| parser)
5. Use lambda for transformations between steps
6. Use streaming: for chunk in chain.stream()
7. Use batch: chain.batch(inputs)
8. Add error handling around invocations

Benefits after migration:
- Simpler code
- Better performance
- Native streaming
- Type safety
- Easier debugging
"""
```

---

## ðŸ“Š Quick Reference: Chain Patterns

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Pattern                 â”‚ Use Case             â”‚ Example                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Sequential              â”‚ Linear workflows     â”‚ analyze â†’ extract â†’ save â”‚
â”‚ Parallel                â”‚ Independent tasks    â”‚ summarize + translate    â”‚
â”‚ Conditional/Branching   â”‚ Decision trees       â”‚ IF type == code THEN ... â”‚
â”‚ RAG                     â”‚ Document QA          â”‚ retrieve â†’ augment â†’ gen â”‚
â”‚ Agent                   â”‚ Autonomous tasks     â”‚ think â†’ decide â†’ act     â”‚
â”‚ Map-Reduce              â”‚ Batch processing     â”‚ map over docs, reduce    â”‚
â”‚ Fan-out/Fan-in          â”‚ Multi-task parallel  â”‚ parallel chains          â”‚
â”‚ Pipeline                â”‚ Multi-stage process  â”‚ step1 â†’ step2 â†’ step3    â”‚
â”‚ Loop/Iterate            â”‚ Repetitive tasks     â”‚ while condition: retry   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## â›“ï¸ Chain Execution Methods Quick Reference

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Method       â”‚ Use Case             â”‚ Example                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ invoke()     â”‚ Single sync call     â”‚ chain.invoke(input)      â”‚
â”‚ stream()     â”‚ Real-time output     â”‚ for chunk in stream()    â”‚
â”‚ batch()      â”‚ Multiple sync        â”‚ chain.batch(inputs)      â”‚
â”‚ ainvoke()    â”‚ Single async         â”‚ await chain.ainvoke()   â”‚
â”‚ astream()    â”‚ Async streaming      â”‚ async for chunk in ...   â”‚
â”‚ abatch()     â”‚ Multiple async       â”‚ await chain.abatch()    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸš€ Production Chain Checklist

- [ ] Using LCEL (not legacy chains)
- [ ] Input/output types are clear
- [ ] Error handling implemented
- [ ] Caching implemented for repeated queries
- [ ] Logging and monitoring in place
- [ ] Performance profiled and optimized
- [ ] Streaming support if needed
- [ ] Batch processing for multiple inputs
- [ ] Fallback chains for reliability
- [ ] Chain composition is clean and readable
- [ ] Unit tests for each chain step
- [ ] Async support where applicable
- [ ] Documentation for complex chains

---

**Master LCEL. Build composable chains. Create production-grade LLM systems.** â›“ï¸âœ¨

# ðŸ” Best Practices & Tips for Managing Retrieval in LangChain
> Document Loaders, Vector Stores, Retrievers, and RAG Mastery Guide

---

## ðŸ“‹ Part 1: Best Practices for Retrieval Management

### 1. **Choose the Right Document Loader for Your Data**

**Why:** Different data sources need different loaders. Using the wrong one = corrupted or missing data.

```python
from langchain_community.document_loaders import (
    TextLoader,
    PyPDFLoader,
    CSVLoader,
    JSONLoader,
    WebBaseLoader,
    DirectoryLoader,
    UnstructuredFileLoader
)

# âœ… 1. TextLoader - Plain text files
loader = TextLoader("document.txt")
docs = loader.load()

# âœ… 2. PyPDFLoader - PDF documents
pdf_loader = PyPDFLoader("document.pdf")
docs = pdf_loader.load()

# âœ… 3. CSVLoader - CSV files with structured data
csv_loader = CSVLoader(file_path="data.csv")
docs = csv_loader.load()

# âœ… 4. JSONLoader - JSON documents
json_loader = JSONLoader(
    file_path="data.json",
    jq_schema=".[]",  # JSON query
    content_key="text"  # Key containing content
)
docs = json_loader.load()

# âœ… 5. WebBaseLoader - Web pages
web_loader = WebBaseLoader(
    web_path="https://example.com"
)
docs = web_loader.load()

# âœ… 6. DirectoryLoader - Multiple files in directory
from langchain_text_splitters import RecursiveCharacterTextSplitter

dir_loader = DirectoryLoader(
    path="./documents",
    glob="**/*.txt",  # Pattern matching
    loader_cls=TextLoader,
    recursive=True
)
docs = dir_loader.load()

# âœ… 7. UnstructuredFileLoader - Automatic format detection
unstructured_loader = UnstructuredFileLoader("mixed_document.pdf")
docs = unstructured_loader.load()

# âœ… Selection guide
loader_selector = {
    ".txt": TextLoader,
    ".pdf": PyPDFLoader,
    ".csv": CSVLoader,
    ".json": JSONLoader,
    "http": WebBaseLoader,
}

def get_loader(file_path: str):
    """Auto-select loader based on file type"""
    if file_path.startswith("http"):
        return WebBaseLoader(file_path)
    
    import os
    ext = os.path.splitext(file_path)[1]
    LoaderClass = loader_selector.get(ext, TextLoader)
    return LoaderClass(file_path)
```

---

### 2. **Implement Proper Document Splitting**

**Why:** Large documents must be split into chunks. Wrong chunk size = lost context or too-small pieces.

```python
from langchain_text_splitters import (
    RecursiveCharacterTextSplitter,
    CharacterTextSplitter,
    TokenTextSplitter,
    MarkdownHeaderTextSplitter,
    Language
)
from langchain_community.document_loaders import TextLoader

loader = TextLoader("document.txt")
docs = loader.load()

# âœ… 1. RecursiveCharacterTextSplitter - Best for most cases
recursive_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,           # Size of each chunk
    chunk_overlap=200,         # Overlap between chunks (maintains context)
    length_function=len,       # How to measure size
    separators=["\n\n", "\n", " ", ""]  # Split by this order
)
chunks = recursive_splitter.split_documents(docs)

# âœ… 2. TokenTextSplitter - Split by tokens (more accurate for LLMs)
from langchain_openai import OpenAIEmbeddings

token_splitter = TokenTextSplitter(
    chunk_size=500,            # Number of tokens
    chunk_overlap=50,          # Token overlap
    encoding_name="cl100k_base"  # OpenAI encoding
)
chunks = token_splitter.split_documents(docs)

# âœ… 3. MarkdownHeaderTextSplitter - Preserve markdown structure
markdown_splitter = MarkdownHeaderTextSplitter(
    headers_to_split_on=[
        ("#", "Header 1"),
        ("##", "Header 2"),
        ("###", "Header 3"),
    ]
)
md_chunks = markdown_splitter.split_text(docs[0].page_content)

# âœ… 4. Language-specific splitting
code_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.PYTHON,
    chunk_size=500,
    chunk_overlap=50
)
code_chunks = code_splitter.split_documents(docs)

# âœ… Optimal chunk size guide
class ChunkSizeSelector:
    @staticmethod
    def select_size(content_type: str) -> int:
        """Select chunk size based on content type"""
        sizes = {
            "dense_text": 512,      # Technical docs, dense content
            "narrative": 1024,      # Stories, articles
            "code": 300,            # Source code
            "structured": 256,      # Tables, CSV
            "web": 800,             # Web content
        }
        return sizes.get(content_type, 1000)
    
    @staticmethod
    def select_overlap(chunk_size: int) -> int:
        """Calculate overlap (typically 10-20% of chunk size)"""
        return max(50, int(chunk_size * 0.2))

# Usage
optimal_size = ChunkSizeSelector.select_size("narrative")
optimal_overlap = ChunkSizeSelector.select_overlap(optimal_size)

splitter = RecursiveCharacterTextSplitter(
    chunk_size=optimal_size,
    chunk_overlap=optimal_overlap
)
```

---

### 3. **Choose the Right Vector Store**

**Why:** Different vector stores have different trade-offs (speed, cost, scale, features).

```python
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import (
    FAISS,
    Chroma,
    Pinecone,
    Weaviate,
    Milvus,
    Qdrant
)

embeddings = OpenAIEmbeddings()

# âœ… 1. FAISS - Fast, local, good for development
from langchain_community.vectorstores import FAISS
vector_store_faiss = FAISS.from_documents(docs, embeddings)
# Best for: Development, offline, small-medium scale
# Pros: Fast, no setup, in-memory
# Cons: Limited to single machine

# âœ… 2. Chroma - Easy to use, persistent
from langchain_community.vectorstores import Chroma
vector_store_chroma = Chroma.from_documents(
    docs,
    embeddings,
    persist_directory="./chroma_db"  # Save to disk
)
# Best for: Prototyping, small scale
# Pros: Simple, persistent, good API
# Cons: Not distributed

# âœ… 3. Pinecone - Managed cloud service
from langchain_community.vectorstores import Pinecone
import pinecone

pinecone.init(api_key="your-api-key", environment="us-west1-gcp")
vector_store_pinecone = Pinecone.from_documents(
    docs,
    embeddings,
    index_name="langchain-index"
)
# Best for: Production, scale
# Pros: Managed, scalable, reliable
# Cons: Requires subscription

# âœ… 4. Weaviate - Open source, production-ready
vector_store_weaviate = Weaviate.from_documents(
    docs,
    embeddings,
    weaviate_url="http://localhost:8080"
)
# Best for: Enterprise, hybrid search
# Pros: Flexible, hybrid search, self-hosted
# Cons: Complex setup

# âœ… 5. Milvus - Scalable, open source
vector_store_milvus = Milvus.from_documents(
    docs,
    embeddings,
    connection_args={"host": "127.0.0.1", "port": 19530}
)
# Best for: Large scale, distributed
# Pros: Scalable, fast, flexible
# Cons: Requires setup

# âœ… 6. Qdrant - Modern, scalable
vector_store_qdrant = Qdrant.from_documents(
    docs,
    embeddings,
    url="http://localhost:6333",
    collection_name="my_collection"
)
# Best for: Production, modern stack
# Pros: Fast, scalable, good API
# Cons: Requires setup

# âœ… Vector store selection matrix
selector = {
    "development": FAISS,
    "prototyping": Chroma,
    "production": Pinecone,  # or Qdrant
    "enterprise": Weaviate,
    "large_scale": Milvus,
    "offline": FAISS,
}

def get_vector_store(use_case: str, docs, embeddings):
    """Select appropriate vector store"""
    StoreClass = selector.get(use_case, FAISS)
    return StoreClass.from_documents(docs, embeddings)
```

---

### 4. **Implement Proper Embeddings Strategy**

**Why:** Embeddings are the core of semantic search. Wrong embeddings = poor retrieval quality.

```python
from langchain_openai import OpenAIEmbeddings
from langchain_community.embeddings import (
    HuggingFaceEmbeddings,
    SentenceTransformerEmbeddings,
)

# âœ… 1. OpenAI Embeddings - Most reliable, paid
openai_embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
# Pros: High quality, reliable, well-supported
# Cons: Requires API key, costs money
# Dimension: 1536 (small) or 3072 (large)

# âœ… 2. HuggingFace Embeddings - Free, local
hf_embeddings = HuggingFaceEmbeddings(
    model_name="all-MiniLM-L6-v2"
)
# Pros: Free, fast, runs locally
# Cons: Lower quality than OpenAI
# Dimension: 384

# âœ… 3. Sentence Transformers - Specialized embeddings
st_embeddings = SentenceTransformerEmbeddings(
    model_name="all-mpnet-base-v2"
)
# Pros: Good quality, fast
# Cons: Requires download
# Dimension: 768

# âœ… Embedding selection guide
class EmbeddingSelector:
    @staticmethod
    def select_for_quality():
        """Best quality (cost money)"""
        return OpenAIEmbeddings(model="text-embedding-3-large")
    
    @staticmethod
    def select_for_speed():
        """Fast, local, free"""
        return HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    
    @staticmethod
    def select_for_domain(domain: str):
        """Domain-specific embeddings"""
        domain_models = {
            "scientific": "allenai/specter",
            "code": "microsoft/codebert-base",
            "biomedical": "dmis-lab/biobert-base-uncased-v1.2",
            "general": "all-mpnet-base-v2",
        }
        model = domain_models.get(domain, "all-mpnet-base-v2")
        return HuggingFaceEmbeddings(model_name=model)

# Usage
quality_embeddings = EmbeddingSelector.select_for_quality()
fast_embeddings = EmbeddingSelector.select_for_speed()
science_embeddings = EmbeddingSelector.select_for_domain("scientific")

# âœ… Caching embeddings for cost savings
import pickle
import hashlib

class EmbeddingCache:
    def __init__(self, embedding_model, cache_file="embeddings.pkl"):
        self.model = embedding_model
        self.cache_file = cache_file
        self.cache = self._load_cache()
    
    def _load_cache(self):
        try:
            with open(self.cache_file, 'rb') as f:
                return pickle.load(f)
        except FileNotFoundError:
            return {}
    
    def embed_documents(self, texts):
        """Embed with caching"""
        embeddings = []
        to_embed = []
        indices = []
        
        for i, text in enumerate(texts):
            hash_key = hashlib.md5(text.encode()).hexdigest()
            
            if hash_key in self.cache:
                embeddings.append(self.cache[hash_key])
            else:
                to_embed.append(text)
                indices.append(i)
        
        # Embed new texts
        if to_embed:
            new_embeddings = self.model.embed_documents(to_embed)
            
            for i, idx in enumerate(indices):
                hash_key = hashlib.md5(to_embed[i].encode()).hexdigest()
                self.cache[hash_key] = new_embeddings[i]
                embeddings.insert(idx, new_embeddings[i])
        
        self._save_cache()
        return embeddings
    
    def _save_cache(self):
        with open(self.cache_file, 'wb') as f:
            pickle.dump(self.cache, f)

# Usage
cached_embeddings = EmbeddingCache(openai_embeddings)
embedded_docs = cached_embeddings.embed_documents([doc.page_content for doc in docs])
```

---

### 5. **Build Effective Retrievers**

**Why:** Retrievers find relevant documents. Poor retrieval = poor answers.

```python
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain.retrievers import (
    BM25Retriever,
    EnsembleRetriever,
    ContextualCompressionRetriever,
    SelfQueryRetriever
)

embeddings = OpenAIEmbeddings()
vector_store = FAISS.from_documents(docs, embeddings)

# âœ… 1. Vector Store Retriever - Semantic search
vector_retriever = vector_store.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 3}  # Return top 3 results
)

# Search types:
# - "similarity": Standard semantic similarity
# - "similarity_score_threshold": Only return above threshold
# - "mmr": Maximize marginal relevance (diversity)

vector_retriever_mmr = vector_store.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 3, "fetch_k": 10}  # Fetch more, return diverse
)

vector_retriever_threshold = vector_store.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={"score_threshold": 0.7, "k": 3}
)

# âœ… 2. BM25Retriever - Keyword/sparse search
bm25_retriever = BM25Retriever.from_documents(docs)

# âœ… 3. Ensemble Retriever - Combine dense and sparse
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_retriever],
    weights=[0.5, 0.5]  # Balance between keyword and semantic
)

# âœ… 4. Contextual Compression - Filter irrelevant docs
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMCompressor
from langchain_openai import ChatOpenAI

compressor = LLMCompressor(
    base_compressor=None,
    llm=ChatOpenAI(model="gpt-4")
)

compression_retriever = ContextualCompressionRetriever(
    base_retriever=vector_retriever,
    base_compressor=compressor
)

# âœ… 5. Self-Query Retriever - Extract filters from query
from langchain.retrievers.self_query.base import SelfQueryRetriever
from langchain.chains.query_constructor.base import AttributeInfo

metadata_field_info = [
    AttributeInfo(name="author", description="Author of document", type="string"),
    AttributeInfo(name="year", description="Year published", type="integer"),
]

self_query_retriever = SelfQueryRetriever.from_llm(
    llm=ChatOpenAI(model="gpt-4"),
    vectorstore=vector_store,
    document_contents="Machine learning research papers",
    metadata_field_info=metadata_field_info,
    verbose=True
)

# âœ… Retriever selection guide
class RetrieverSelector:
    @staticmethod
    def for_semantic_search():
        """Best for conceptual similarity"""
        return vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 5})
    
    @staticmethod
    def for_keyword_search():
        """Best for exact term matching"""
        return BM25Retriever.from_documents(docs)
    
    @staticmethod
    def for_diversity():
        """Return diverse results (avoid redundancy)"""
        return vector_store.as_retriever(
            search_type="mmr",
            search_kwargs={"k": 5, "fetch_k": 20}
        )
    
    @staticmethod
    def for_hybrid_search():
        """Combine keyword and semantic"""
        return EnsembleRetriever(
            retrievers=[BM25Retriever.from_documents(docs), vector_store.as_retriever()],
            weights=[0.4, 0.6]
        )
    
    @staticmethod
    def for_filtered_search():
        """With metadata filtering"""
        return vector_store.as_retriever(
            search_kwargs={"k": 5, "filter": {"category": "tech"}}
        )

# Usage
retriever = RetrieverSelector.for_hybrid_search()
results = retriever.invoke("What is machine learning?")
```

---

### 6. **Build Production-Ready RAG Chains**

**Why:** RAG chains are the heart of retrieval. Structure matters for reliability and quality.

```python
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from operator import itemgetter

# Setup
loader = TextLoader("document.txt")
docs = loader.load()

embeddings = OpenAIEmbeddings()
vector_store = FAISS.from_documents(docs, embeddings)
retriever = vector_store.as_retriever(search_kwargs={"k": 3})

llm = ChatOpenAI(model="gpt-4")

# âœ… Basic RAG chain
basic_rag = (
    {
        "context": retriever | (lambda docs: "\n".join([d.page_content for d in docs])),
        "question": itemgetter("question")
    }
    | ChatPromptTemplate.from_template(
        """Use context to answer question.
        
Context: {context}
Question: {question}
Answer:"""
    )
    | llm
    | StrOutputParser()
)

# âœ… Advanced RAG with metadata
class AdvancedRAG:
    def __init__(self, retriever, llm):
        self.retriever = retriever
        self.llm = llm
    
    def invoke(self, question: str):
        """Execute RAG with metadata"""
        
        # Retrieve documents with metadata
        docs = self.retriever.invoke(question)
        
        # Format context with metadata
        context = "\n".join([
            f"[{doc.metadata.get('source', 'Unknown')}] {doc.page_content}"
            for doc in docs
        ])
        
        # Generate answer
        prompt = ChatPromptTemplate.from_template(
            """Answer question using provided context.
            
Context:
{context}

Question: {question}

Answer:"""
        )
        
        result = (prompt | self.llm).invoke({
            "context": context,
            "question": question
        })
        
        return {
            "answer": result.content,
            "sources": [doc.metadata.get("source") for doc in docs]
        }

# Usage
rag = AdvancedRAG(retriever, llm)
result = rag.invoke("What is the main topic?")
print(f"Answer: {result['answer']}")
print(f"Sources: {result['sources']}")

# âœ… RAG with query refinement
class RefinedRAG:
    def __init__(self, retriever, llm):
        self.retriever = retriever
        self.llm = llm
    
    def refine_query(self, original_query: str) -> str:
        """Improve query for better retrieval"""
        
        prompt = ChatPromptTemplate.from_template(
            """Rephrase this query to improve search results:
Original: {query}
Refined:"""
        )
        
        result = (prompt | self.llm).invoke({"query": original_query})
        return result.content
    
    def invoke(self, question: str):
        """RAG with query refinement"""
        
        # Refine query
        refined = self.refine_query(question)
        
        # Retrieve with refined query
        docs = self.retriever.invoke(refined)
        
        context = "\n".join([d.page_content for d in docs])
        
        # Generate answer
        prompt = ChatPromptTemplate.from_template(
            """Use context to answer original question.
            
Context: {context}
Original question: {question}
Answer:"""
        )
        
        result = (prompt | self.llm).invoke({
            "context": context,
            "question": question
        })
        
        return result.content

refined_rag = RefinedRAG(retriever, llm)
answer = refined_rag.invoke("How do I use LangChain chains?")
```

---

### 7. **Implement Metadata Management**

**Why:** Metadata enables filtering, source tracking, and better retrieval.

```python
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
import json
from datetime import datetime

# âœ… 1. Add metadata during loading
docs_with_metadata = [
    Document(
        page_content=doc.page_content,
        metadata={
            "source": "document.txt",
            "author": "John Doe",
            "year": 2024,
            "category": "AI",
            "page": i,
            "loaded_at": datetime.now().isoformat()
        }
    )
    for i, doc in enumerate(docs)
]

# âœ… 2. Filter by metadata
vector_store = FAISS.from_documents(docs_with_metadata, OpenAIEmbeddings())

# Search with filters
retriever = vector_store.as_retriever(
    search_kwargs={
        "k": 5,
        "filter": {"year": 2024, "category": "AI"}  # Filter criteria
    }
)

# âœ… 3. Metadata schema management
class MetadataManager:
    def __init__(self):
        self.schema = {}
    
    def define_schema(self, field_name: str, field_type: str, required: bool = False):
        """Define expected metadata fields"""
        self.schema[field_name] = {
            "type": field_type,
            "required": required
        }
    
    def validate_metadata(self, metadata: dict) -> bool:
        """Validate metadata against schema"""
        
        for field, spec in self.schema.items():
            if spec["required"] and field not in metadata:
                return False
            
            if field in metadata:
                if not isinstance(metadata[field], self._get_type(spec["type"])):
                    return False
        
        return True
    
    def _get_type(self, type_name: str):
        """Map type name to Python type"""
        types = {
            "string": str,
            "integer": int,
            "float": float,
            "boolean": bool,
            "list": list,
        }
        return types.get(type_name, str)
    
    def add_metadata(self, doc: Document, additional_meta: dict):
        """Add metadata to document"""
        if not self.validate_metadata(additional_meta):
            raise ValueError("Invalid metadata")
        
        doc.metadata.update(additional_meta)

# Usage
manager = MetadataManager()
manager.define_schema("source", "string", required=True)
manager.define_schema("author", "string", required=True)
manager.define_schema("year", "integer", required=False)

new_doc = Document(page_content="Sample content", metadata={})
manager.add_metadata(new_doc, {"source": "file.txt", "author": "Jane"})
```

---

### 8. **Implement Retrieval Evaluation and Monitoring**

**Why:** You can't improve what you don't measure. Evaluate retrieval quality.

```python
from typing import List, Tuple
import numpy as np

class RetrieverEvaluator:
    def __init__(self):
        self.metrics = []
    
    def evaluate_relevance(
        self,
        query: str,
        retrieved_docs: List,
        relevant_docs: List
    ) -> float:
        """Calculate precision and recall"""
        
        retrieved_ids = set(doc.metadata.get("id") for doc in retrieved_docs)
        relevant_ids = set(doc.metadata.get("id") for doc in relevant_docs)
        
        if len(retrieved_ids) == 0:
            return 0.0
        
        true_positives = len(retrieved_ids & relevant_ids)
        
        # Precision: relevant out of retrieved
        precision = true_positives / len(retrieved_ids)
        
        # Recall: relevant out of all relevant
        recall = true_positives / len(relevant_ids) if relevant_ids else 0
        
        # F1 score
        if precision + recall == 0:
            f1 = 0
        else:
            f1 = 2 * (precision * recall) / (precision + recall)
        
        return {
            "precision": precision,
            "recall": recall,
            "f1": f1
        }
    
    def evaluate_ranking(self, retrieved_docs: List, relevant_docs: List) -> float:
        """Mean Reciprocal Rank - how early relevant doc appears"""
        
        relevant_ids = set(doc.metadata.get("id") for doc in relevant_docs)
        
        for i, doc in enumerate(retrieved_docs):
            if doc.metadata.get("id") in relevant_ids:
                return 1 / (i + 1)
        
        return 0.0
    
    def evaluate_ndcg(self, retrieved_docs: List, relevant_docs: List, k: int = 5) -> float:
        """Normalized Discounted Cumulative Gain"""
        
        relevant_ids = set(doc.metadata.get("id") for doc in relevant_docs)
        
        # Calculate DCG
        dcg = 0
        for i, doc in enumerate(retrieved_docs[:k]):
            if doc.metadata.get("id") in relevant_ids:
                dcg += 1 / np.log2(i + 2)
        
        # Calculate IDCG (ideal DCG)
        idcg = sum(1 / np.log2(i + 2) for i in range(min(k, len(relevant_ids))))
        
        return dcg / idcg if idcg > 0 else 0
    
    def evaluate_retriever(
        self,
        retriever,
        test_cases: List[Tuple[str, List]]
    ) -> dict:
        """Comprehensive retrieval evaluation"""
        
        results = {
            "precision": [],
            "recall": [],
            "f1": [],
            "mrr": [],
            "ndcg": []
        }
        
        for query, relevant_docs in test_cases:
            retrieved = retriever.invoke(query)
            
            metrics = self.evaluate_relevance(query, retrieved, relevant_docs)
            results["precision"].append(metrics["precision"])
            results["recall"].append(metrics["recall"])
            results["f1"].append(metrics["f1"])
            
            results["mrr"].append(self.evaluate_ranking(retrieved, relevant_docs))
            results["ndcg"].append(self.evaluate_ndcg(retrieved, relevant_docs))
        
        # Average all metrics
        return {
            key: np.mean(values) for key, values in results.items()
        }

# Usage
evaluator = RetrieverEvaluator()

test_cases = [
    ("What is AI?", [doc1, doc2, doc3]),
    ("Machine learning?", [doc4, doc5]),
]

scores = evaluator.evaluate_retriever(retriever, test_cases)
print(f"F1: {scores['f1']:.3f}")
print(f"NDCG: {scores['ndcg']:.3f}")
```

---

### 9. **Implement Efficient Indexing and Caching**

**Why:** Indexing large document collections is expensive. Cache aggressively.

```python
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
import pickle
import hashlib
import os
from datetime import datetime, timedelta

class IndexManager:
    def __init__(self, cache_dir: str = ".vector_cache"):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
    
    def _get_cache_path(self, collection_name: str) -> str:
        """Get cache file path"""
        return os.path.join(self.cache_dir, f"{collection_name}.pkl")
    
    def _get_metadata_path(self, collection_name: str) -> str:
        """Get metadata file path"""
        return os.path.join(self.cache_dir, f"{collection_name}_meta.json")
    
    def build_or_load_index(
        self,
        collection_name: str,
        docs,
        embeddings,
        force_rebuild: bool = False
    ):
        """Build index or load from cache"""
        
        cache_path = self._get_cache_path(collection_name)
        
        # Load from cache if exists and not forced
        if os.path.exists(cache_path) and not force_rebuild:
            print(f"Loading index from cache: {collection_name}")
            with open(cache_path, 'rb') as f:
                return pickle.load(f)
        
        # Build new index
        print(f"Building new index: {collection_name}")
        vector_store = FAISS.from_documents(docs, embeddings)
        
        # Save to cache
        with open(cache_path, 'wb') as f:
            pickle.dump(vector_store, f)
        
        # Save metadata
        import json
        with open(self._get_metadata_path(collection_name), 'w') as f:
            json.dump({
                "created_at": datetime.now().isoformat(),
                "doc_count": len(docs)
            }, f)
        
        return vector_store
    
    def add_documents(self, collection_name: str, new_docs, embeddings):
        """Add documents to existing index"""
        
        cache_path = self._get_cache_path(collection_name)
        
        if os.path.exists(cache_path):
            with open(cache_path, 'rb') as f:
                vector_store = pickle.load(f)
        else:
            from langchain_community.vectorstores import FAISS
            vector_store = FAISS.from_documents([], embeddings)
        
        # Add new documents
        vector_store.add_documents(new_docs)
        
        # Save updated index
        with open(cache_path, 'wb') as f:
            pickle.dump(vector_store, f)
        
        return vector_store
    
    def clear_cache(self, collection_name: str = None):
        """Clear cache"""
        if collection_name:
            for suffix in ["", "_meta"]:
                path = self._get_cache_path(collection_name) + suffix
                if os.path.exists(path):
                    os.remove(path)
        else:
            import shutil
            shutil.rmtree(self.cache_dir)
            os.makedirs(self.cache_dir)

# Usage
manager = IndexManager()

# Load or build
vector_store = manager.build_or_load_index(
    "my_documents",
    docs,
    embeddings
)

# Add more docs later
vector_store = manager.add_documents("my_documents", new_docs, embeddings)
```

---

### 10. **Implement Multi-Index Retrieval**

**Why:** Different document collections need different indexes. Manage them efficiently.

```python
from typing import Dict, List
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings

class MultiIndexRetriever:
    def __init__(self):
        self.indexes: Dict[str, FAISS] = {}
        self.embeddings = OpenAIEmbeddings()
    
    def add_collection(self, collection_name: str, docs):
        """Add a document collection"""
        vector_store = FAISS.from_documents(docs, self.embeddings)
        self.indexes[collection_name] = vector_store
    
    def retrieve_from_collection(
        self,
        query: str,
        collection_name: str,
        k: int = 3
    ):
        """Retrieve from specific collection"""
        
        if collection_name not in self.indexes:
            raise ValueError(f"Collection {collection_name} not found")
        
        retriever = self.indexes[collection_name].as_retriever(
            search_kwargs={"k": k}
        )
        return retriever.invoke(query)
    
    def retrieve_from_all(self, query: str, k: int = 3) -> Dict[str, List]:
        """Retrieve from all collections"""
        
        results = {}
        for collection_name, vector_store in self.indexes.items():
            retriever = vector_store.as_retriever(
                search_kwargs={"k": k}
            )
            results[collection_name] = retriever.invoke(query)
        
        return results
    
    def retrieve_with_routing(
        self,
        query: str,
        route_fn,
        k: int = 3
    ):
        """Intelligently route query to appropriate collection"""
        
        collection = route_fn(query)
        return self.retrieve_from_collection(query, collection, k)
    
    def get_collections(self) -> List[str]:
        """List available collections"""
        return list(self.indexes.keys())

# Usage
multi_retriever = MultiIndexRetriever()

# Add collections
multi_retriever.add_collection("research_papers", research_docs)
multi_retriever.add_collection("documentation", docs_docs)
multi_retriever.add_collection("news_articles", news_docs)

# Retrieve from specific
results = multi_retriever.retrieve_from_collection(
    "What is neural networks?",
    "research_papers"
)

# Retrieve from all
all_results = multi_retriever.retrieve_from_all("Machine learning")

# Smart routing
def route_query(query: str) -> str:
    """Route query to appropriate collection"""
    if "research" in query.lower():
        return "research_papers"
    elif "how" in query.lower():
        return "documentation"
    else:
        return "news_articles"

routed_results = multi_retriever.retrieve_with_routing(
    "How do I use transformers?",
    route_fn=route_query
)
```

---

## ðŸŽ¯ Part 2: Tips & Tricks for Retrieval

### Trick 1: **Implement Hybrid Search (Dense + Sparse)**

Combine semantic and keyword search for better results.

```python
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings

# Create both retrievers
bm25_retriever = BM25Retriever.from_documents(docs)
vector_retriever = FAISS.from_documents(docs, OpenAIEmbeddings()).as_retriever()

# Combine with weights
hybrid_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_retriever],
    weights=[0.4, 0.6]  # Adjust balance
)

# Usage
results = hybrid_retriever.invoke("What is machine learning?")
print(f"Retrieved {len(results)} documents")

# Custom weighted ensemble
class WeightedEnsemble:
    def __init__(self, retrievers, weights):
        self.retrievers = retrievers
        self.weights = weights
    
    def invoke(self, query: str, k: int = 5):
        """Get results from all, merge with weights"""
        
        all_results = {}
        
        for retriever, weight in zip(self.retrievers, self.weights):
            results = retriever.invoke(query)
            
            for doc in results:
                doc_id = id(doc)
                if doc_id not in all_results:
                    all_results[doc_id] = {"doc": doc, "score": 0}
                all_results[doc_id]["score"] += weight
        
        # Sort by weighted score
        sorted_results = sorted(
            all_results.values(),
            key=lambda x: x["score"],
            reverse=True
        )
        
        return [r["doc"] for r in sorted_results[:k]]
```

---

### Trick 2: **Implement Query Expansion**

Generate multiple queries to improve retrieval.

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

class QueryExpander:
    def __init__(self, llm):
        self.llm = llm
    
    def expand_query(self, query: str, num_variations: int = 3) -> list:
        """Generate query variations"""
        
        prompt = ChatPromptTemplate.from_template(
            """Generate {n} variations of this query for better search:
Query: {query}

Variations (one per line):"""
        )
        
        response = (prompt | self.llm).invoke({
            "query": query,
            "n": num_variations
        })
        
        # Parse variations
        variations = response.content.strip().split('\n')
        return [query] + [v.strip() for v in variations if v.strip()]

# Usage
expander = QueryExpander(ChatOpenAI(model="gpt-4"))

original = "How do I use LangChain?"
expanded = expander.expand_query(original)

# Retrieve with all queries
all_docs = set()
for q in expanded:
    results = retriever.invoke(q)
    all_docs.update(results)
```

---

### Trick 3: **Implement Re-ranking**

Reorder retrieved documents by relevance.

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
import numpy as np

class DocumentReranker:
    def __init__(self, llm):
        self.llm = llm
    
    def rerank(self, query: str, documents: list, top_k: int = 3):
        """Rerank documents by relevance to query"""
        
        scores = []
        
        for doc in documents:
            prompt = ChatPromptTemplate.from_template(
                """Rate relevance of document to query (0-10):
Query: {query}
Document: {doc}
Score:"""
            )
            
            response = (prompt | self.llm).invoke({
                "query": query,
                "doc": doc.page_content[:500]
            })
            
            try:
                score = float(response.content.strip().split()[-1])
                scores.append(score)
            except:
                scores.append(0)
        
        # Sort by score
        ranked = sorted(
            zip(documents, scores),
            key=lambda x: x[1],
            reverse=True
        )
        
        return [doc for doc, _ in ranked[:top_k]]

# Usage
reranker = DocumentReranker(ChatOpenAI(model="gpt-4"))
initial_results = retriever.invoke("query")
reranked = reranker.rerank("query", initial_results, top_k=3)
```

---

### Trick 4: **Implement Contextual Retrieval**

Expand retrieved chunks with surrounding context.

```python
class ContextualDocumentRetriever:
    def __init__(self, retriever, vector_store):
        self.retriever = retriever
        self.vector_store = vector_store
    
    def retrieve_with_context(self, query: str, context_size: int = 2):
        """Retrieve docs with surrounding context"""
        
        # Get base results
        results = self.retriever.invoke(query)
        
        expanded_results = []
        for doc in results:
            # Get metadata with position
            source = doc.metadata.get("source")
            page = doc.metadata.get("page", 0)
            
            # Retrieve surrounding documents
            surrounding = self.vector_store.similarity_search(
                query,
                k=context_size * 2
            )
            
            # Build context
            context_docs = [d for d in surrounding 
                           if d.metadata.get("source") == source]
            
            # Combine with original
            combined_content = "\n---\n".join([
                d.page_content for d in context_docs
            ])
            
            # Create expanded document
            expanded_doc = doc.__class__(
                page_content=combined_content,
                metadata=doc.metadata
            )
            expanded_results.append(expanded_doc)
        
        return expanded_results
```

---

### Trick 5: **Implement Dynamic k Selection**

Adjust number of retrieved documents based on query complexity.

```python
class DynamicRetriever:
    def __init__(self, retriever, llm):
        self.retriever = retriever
        self.llm = llm
    
    def estimate_complexity(self, query: str) -> float:
        """Estimate query complexity (0-1)"""
        
        # Simple heuristics
        complexity = 0
        
        # Multi-part questions
        if "and" in query.lower() or "?" in query:
            complexity += 0.3
        
        # Specific keywords
        if any(w in query.lower() for w in ["complex", "detailed", "explain", "how"]):
            complexity += 0.4
        
        # Length (longer = more complex)
        complexity += min(0.3, len(query) / 100)
        
        return min(1.0, complexity)
    
    def invoke(self, query: str):
        """Retrieve with dynamic k"""
        
        complexity = self.estimate_complexity(query)
        k = max(3, int(10 * complexity))  # 3-10 docs based on complexity
        
        retriever_with_k = self.retriever.invoke(query)
        return retriever_with_k[:k]

# Usage
dynamic_retriever = DynamicRetriever(retriever, llm)
results = dynamic_retriever.invoke("What is AI?")  # Simple, few docs
results = dynamic_retriever.invoke("Explain deep learning, transformers, and attention mechanisms")  # Complex, more docs
```

---

### Trick 6: **Implement Lost-in-Middle Detection**

Address the "lost in the middle" problem where relevant info is buried.

```python
class LostInMiddleHandler:
    def __init__(self, retriever):
        self.retriever = retriever
    
    def invoke_with_rearrangement(self, query: str):
        """Arrange docs to put most relevant in middle"""
        
        docs = self.retriever.invoke(query)
        
        # Sort: high relevance â†’ middle, low relevance â†’ edges
        n = len(docs)
        
        if n <= 2:
            return docs
        
        # Arrange: center, then alternate left/right
        rearranged = [None] * n
        rearranged[n // 2] = docs[0]  # Most relevant in middle
        
        left, right = n // 2 - 1, n // 2 + 1
        
        for i in range(1, n):
            if left >= 0:
                rearranged[left] = docs[i]
                left -= 1
            elif right < n:
                rearranged[right] = docs[i]
                right += 1
        
        return [d for d in rearranged if d is not None]
```

---

### Trick 7: **Implement Retrieval Caching**

Cache retrieval results to reduce API calls.

```python
import hashlib
import json
from datetime import datetime, timedelta

class CachedRetriever:
    def __init__(self, retriever, ttl_minutes: int = 60):
        self.retriever = retriever
        self.cache = {}
        self.ttl = timedelta(minutes=ttl_minutes)
    
    def invoke(self, query: str):
        """Retrieve with caching"""
        
        cache_key = hashlib.md5(query.encode()).hexdigest()
        
        if cache_key in self.cache:
            cached_data = self.cache[cache_key]
            if datetime.now() - cached_data["time"] < self.ttl:
                return cached_data["results"]
        
        # Cache miss
        results = self.retriever.invoke(query)
        
        self.cache[cache_key] = {
            "results": results,
            "time": datetime.now()
        }
        
        return results
    
    def clear_expired(self):
        """Remove expired cache entries"""
        now = datetime.now()
        expired = [
            k for k, v in self.cache.items()
            if now - v["time"] > self.ttl
        ]
        
        for k in expired:
            del self.cache[k]

# Usage
cached = CachedRetriever(retriever, ttl_minutes=30)
results = cached.invoke("query")  # Cached after first call
```

---

### Trick 8: **Implement Retrieval with Fallback**

Fallback to alternative retrieval if primary fails.

```python
class FallbackRetriever:
    def __init__(self, primary, fallback):
        self.primary = primary
        self.fallback = fallback
    
    def invoke(self, query: str, k: int = 3):
        """Try primary, fallback if needed"""
        
        try:
            results = self.primary.invoke(query)
            
            if not results or len(results) == 0:
                print("Primary retriever returned no results, using fallback")
                return self.fallback.invoke(query)
            
            return results
        
        except Exception as e:
            print(f"Primary retriever failed: {e}, using fallback")
            return self.fallback.invoke(query)

# Usage
bm25_retriever = BM25Retriever.from_documents(docs)
vector_retriever = FAISS.from_documents(docs, embeddings).as_retriever()

fallback = FallbackRetriever(vector_retriever, bm25_retriever)
results = fallback.invoke("query")
```

---

### Trick 9: **Implement Diversity-Aware Retrieval**

Avoid redundant results.

```python
from sklearn.pairwise_distances import cosine_distances
import numpy as np

class DiverseRetriever:
    def __init__(self, retriever, embeddings):
        self.retriever = retriever
        self.embeddings = embeddings
    
    def invoke(self, query: str, k: int = 5, diversity: float = 0.7):
        """Retrieve diverse documents"""
        
        # Get initial results
        initial_results = self.retriever.invoke(query)
        
        if len(initial_results) <= k:
            return initial_results
        
        # Embed all documents
        embeddings = [
            self.embeddings.embed_query(doc.page_content)
            for doc in initial_results
        ]
        
        selected = [0]  # Start with first result
        
        # Greedily select diverse documents
        while len(selected) < k:
            best_idx = None
            best_score = -1
            
            for i in range(len(initial_results)):
                if i in selected:
                    continue
                
                # Calculate distance to selected
                distances = [
                    cosine_distances([embeddings[i]], [embeddings[j]])[0][0]
                    for j in selected
                ]
                
                min_distance = min(distances)
                
                if min_distance > best_score:
                    best_score = min_distance
                    best_idx = i
            
            if best_idx is not None:
                selected.append(best_idx)
        
        return [initial_results[i] for i in selected]
```

---

### Trick 10: **Implement Adaptive Retrieval**

Adjust retrieval strategy based on results quality.

```python
class AdaptiveRetriever:
    def __init__(self, base_retriever, llm):
        self.base_retriever = base_retriever
        self.llm = llm
    
    def score_results(self, query: str, results: list) -> float:
        """Score quality of retrieval results"""
        
        if not results:
            return 0.0
        
        # Check if top result is relevant
        prompt = ChatPromptTemplate.from_template(
            """Is this document relevant to the query? (yes/no)
Query: {query}
Document: {doc}
Answer:"""
        )
        
        response = (prompt | self.llm).invoke({
            "query": query,
            "doc": results[0].page_content[:300]
        })
        
        is_relevant = "yes" in response.content.lower()
        return 0.8 if is_relevant else 0.2
    
    def invoke(self, query: str):
        """Adaptively retrieve"""
        
        # Try with default k
        results = self.base_retriever.invoke(query)
        score = self.score_results(query, results)
        
        # If low quality, retry with query expansion
        if score < 0.5:
            # Expand query
            expanded = f"{query} {query.upper()} additional context"
            results = self.base_retriever.invoke(expanded)
        
        return results
```

---

## ðŸ“Š Quick Reference: Retrieval Methods

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Retrieval Method   â”‚ Best For         â”‚ Pros          â”‚ Cons        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Vector Similarity  â”‚ Semantic search  â”‚ Fast, accurateâ”‚ Slow to build|
â”‚ BM25               â”‚ Keyword search   â”‚ Fast, simple  â”‚ No semanticsâ”‚
â”‚ Hybrid (Ensemble)  â”‚ General purpose  â”‚ Balanced      â”‚ Slower      â”‚
â”‚ Contextual         â”‚ Documents        â”‚ Context-aware â”‚ Complex     â”‚
â”‚ Reranking          â”‚ Quality boost    â”‚ Better resultsâ”‚ Extra LLM   â”‚
â”‚ Query Expansion    â”‚ Complex queries  â”‚ Comprehensive â”‚ More calls  â”‚
â”‚ Self-Query         â”‚ Filtered search  â”‚ Intelligent   â”‚ Slower      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ” Retrieval Optimization Checklist

- [ ] Document loader matches data format
- [ ] Chunk size optimized for domain
- [ ] Chunk overlap preserves context
- [ ] Embeddings model chosen for quality/speed tradeoff
- [ ] Vector store selected for scale needs
- [ ] Retriever type matches use case
- [ ] Metadata managed and indexed
- [ ] Hybrid search implemented if needed
- [ ] Query expansion for complex queries
- [ ] Reranking for quality boost
- [ ] Caching for repeated queries
- [ ] Fallback retriever for reliability
- [ ] Evaluation metrics tracked

---

## ðŸš€ Production Retrieval Checklist

- [ ] Document loading tested with all file types
- [ ] Chunk strategy validated with test queries
- [ ] Embeddings cached or local
- [ ] Vector store backed up regularly
- [ ] Metadata validated and consistent
- [ ] Retrieval quality monitored
- [ ] Query latency measured
- [ ] Fallback strategies in place
- [ ] Multi-index retrieval tested
- [ ] Re-ranking quality validated
- [ ] Error handling comprehensive
- [ ] Load testing completed
- [ ] Scaling strategy documented

---

**Master retrieval. Build powerful RAG systems. Provide accurate, grounded answers.** ðŸ”âœ¨
